{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural networks from scratch\n",
        "\n",
        "## Libraries and dataset"
      ],
      "id": "b042e32f-a5bc-4104-bb96-08e05f4071d2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "n_classes = 4\n",
        "n_loops = 1\n",
        "n_samples = 1500\n",
        "\n",
        "def spirals(n_classes=3, n_samples=1500, n_loops=2):\n",
        "    klass = np.random.choice(n_classes, n_samples)\n",
        "    radius = np.random.rand(n_samples)\n",
        "    theta = klass * 2 * math.pi / n_classes + radius * 2 * math.pi * n_loops\n",
        "    radius = radius + 0.05 * np.random.randn(n_samples)\n",
        "    return np.column_stack((radius * np.cos(theta), radius * np.sin(theta))).astype(\"float32\"), klass\n",
        "\n",
        "X, y = spirals(n_samples=n_samples, n_classes=n_classes, n_loops=n_loops)"
      ],
      "id": "feb59fae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the dataset"
      ],
      "id": "bb4014a1-6412-4cb6-b135-12321d2f09a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ],
      "id": "fdc5a262"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Activation functions\n",
        "\n",
        "ReLU and sigmoid function and their derivative (should work for numpy\n",
        "array of any dimension (1D, 2D,…))"
      ],
      "id": "36aa9eb0-114f-4ede-82f7-d50650ecbcdb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(v):\n",
        "    ...\n",
        "\n",
        "\n",
        "def drelu(v):\n",
        "    ...\n",
        "\n",
        "\n",
        "def sigmoid(v):\n",
        "    ...\n",
        "\n",
        "\n",
        "def dsigmoid(v):\n",
        "    ..."
      ],
      "id": "45015798"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Multilayer Perceptron\n",
        "\n",
        "First define the shape of the multilayer perceptron:\n",
        "\n",
        "-   `n0`: size of input,\n",
        "-   `n1`: size of hidden layer,\n",
        "-   `n2`: size of output."
      ],
      "id": "d92f8257-c161-49ab-a953-ea9db0bd34cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n0 = ...\n",
        "n1 = ...\n",
        "n2 = ..."
      ],
      "id": "88b38e53"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variables for weights, biases of each layers and gradients of loss wrt\n",
        "to any intermediate quantity."
      ],
      "id": "e9aa17d6-5daf-4b6d-87d4-f42ebd4647d2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random weights\n",
        "W1 = np.random.randn(n0, n1)\n",
        "W2 = np.random.randn(n1, n2)\n",
        "\n",
        "# Biases set to zero\n",
        "b1 = np.zeros((n1, 1))\n",
        "b2 = np.zeros((n2, 1))\n",
        "\n",
        "# Gradients of loss\n",
        "Lx_2 = np.zeros((n2, 1))\n",
        "LW_2 = np.zeros((n1, n2))\n",
        "Lb_2 = np.zeros((n2, 1))\n",
        "\n",
        "Lx_1 = np.zeros((n1, 1))\n",
        "LW_1 = np.zeros((n0, n1))\n",
        "Lb_1 = np.zeros((n1, 1))"
      ],
      "id": "0ef6d8c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What about He’s initialization for `W1` and `W2`?"
      ],
      "id": "1ce7518e-486f-432a-9675-f175b47432d7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random weights with He's initialization\n",
        "W1 = ...\n",
        "W2 = ..."
      ],
      "id": "367692fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the learning rate and the activation functions along their\n",
        "derivatives at each layer:\n",
        "\n",
        "-   `eta`: learning rate\n",
        "-   `af`, `daf`: activation function and its derivative for hidden layer"
      ],
      "id": "f0485e2a-f534-4fa2-b211-c19c3f8ffa19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define eta, af, daf\n",
        "eta = ...\n",
        "af = ...\n",
        "daf = ..."
      ],
      "id": "07ca070e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The learning loop (no minibatch)"
      ],
      "id": "9d85e3b9-b722-4f04-9a5e-b964d014d5a6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nepochs = 15\n",
        "for epoch in range(nepochs + 1):\n",
        "    acc_epoch = 0\n",
        "\n",
        "    # Here we are using stochastic gradient descent (minibatch of size 1)\n",
        "    for idx, (x0, y2) in enumerate(zip(X, y)):\n",
        "        x0 = x0.reshape((-1, 1))\n",
        "\n",
        "        # Implement the forward pass: use `W1`, `x0`, `b1`, `af`, `W2`, `x1`,\n",
        "        # `b2` to define `z1`, `x1`, `z2`, `x2`. Remember that there is no\n",
        "        # activation function for the last layer\n",
        "        z1 = ...\n",
        "        x1 = ...\n",
        "        z2 = ...\n",
        "        x2 = ...\n",
        "\n",
        "        # Predicted class\n",
        "        pred = np.argmax(x2)\n",
        "        acc_epoch += (pred == y2)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            print(f\"Epoch: {epoch:02}, sample: {idx:04}, class: {y2}, pred: {pred}, output: {x2}\")\n",
        "\n",
        "        # One-hot encoding of class `y2`\n",
        "        y2_one_hot = np.zeros((n2, 1))\n",
        "        y2_one_hot[y2, 0] = 1\n",
        "\n",
        "        # Softmax of output needed in the loss\n",
        "        softmax_x2 = np.exp(x2) / sum(np.exp(x2))\n",
        "\n",
        "        # Gradient of loss wrt output layer\n",
        "        Lx_2 = ...\n",
        "\n",
        "        # Gradient of loss wrt weights and biases in second layer\n",
        "        LW_2 = ...\n",
        "        Lb_2 = ...\n",
        "\n",
        "        # Gradient of loss wrt first layer\n",
        "        Lx_1 = ...\n",
        "\n",
        "        # Gradient of loss wrt weights and biases in first layer\n",
        "        LW_1 = ...\n",
        "        Lb_1 = ...\n",
        "\n",
        "        # Gradient descent step: use `eta`, `Lw_1` `Lw_2` `Lb_1` `Lb_2` to\n",
        "        # update `W1`, `W2`, `b1`, `b2`.\n",
        "        W1 -= ...\n",
        "        W2 -= ...\n",
        "        b1 -= ...\n",
        "        b2 -= ...\n",
        "\n",
        "    print(f\"Epoch: {epoch:02}, training accuracy: {acc_epoch/n_samples}\")"
      ],
      "id": "23922efa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization"
      ],
      "id": "2bfef7e3-3099-4ad8-a9dc-1326a91dde2e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num = 250\n",
        "xx = np.linspace(X[:, 0].min(), X[:, 0].max(), num)\n",
        "yy = np.linspace(X[:, 1].min(), X[:, 1].max(), num)\n",
        "XX, YY = np.meshgrid(xx, yy)\n",
        "points = np.c_[XX.ravel(), YY.ravel()]\n",
        "\n",
        "# Forward pass on all points\n",
        "z1 = W1.T @ points.T + b1\n",
        "x1 = af(z1)\n",
        "z2 = W2.T @ x1 + b2\n",
        "x2_hat = np.argmax(z2, axis=0)\n",
        "\n",
        "C = x2_hat.reshape(num, num)\n",
        "\n",
        "plt.contourf(XX, YY, C, cmap=plt.get_cmap(\"tab10\"), alpha=.4)\n",
        "plt.scatter(*X.T, c=y, cmap=plt.get_cmap(\"tab10\"))\n",
        "\n",
        "plt.show()"
      ],
      "id": "9acb464e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The learning loop with minibatch"
      ],
      "id": "7e6066d5-2139-4b45-a17a-8737c5988d25"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n0 = 2\n",
        "n1 = 100\n",
        "n2 = n_classes\n",
        "\n",
        "nepochs = 1000\n",
        "batch_size = 64\n",
        "\n",
        "# Random weights\n",
        "W1 = np.random.randn(n0, n1)\n",
        "W2 = np.random.randn(n1, n2)\n",
        "\n",
        "# Biases set to zero\n",
        "b1 = np.zeros(n1)\n",
        "b2 = np.zeros(n2)\n",
        "\n",
        "# Gradients of loss\n",
        "LX_2 = np.zeros((batch_size, n2))\n",
        "LW_2 = np.zeros((batch_size, n1, n2))\n",
        "Lb_2 = np.zeros((batch_size, n2))\n",
        "\n",
        "LX_1 = np.zeros((batch_size, n1))\n",
        "LW_1 = np.zeros((batch_size, n0, n1))\n",
        "Lb_1 = np.zeros((batch_size, n1))\n",
        "\n",
        "\n",
        "def fake_dataloader(X, y, batch_size=32, shuffle=True):\n",
        "    n_samples = X.shape[0]\n",
        "    indices = np.arange(n_samples)\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        batch_indices = indices[start:end]\n",
        "        yield X[batch_indices], y[batch_indices]\n",
        "\n",
        "\n",
        "for epoch in range(nepochs + 1):\n",
        "    acc_epoch = 0\n",
        "\n",
        "    for idx, (X0, y2) in enumerate(fake_dataloader(X, y, batch_size=batch_size)):\n",
        "        # Implement the forward pass: use `W1`, `X0`, `b1`, `af`, `W2`, `X1`,\n",
        "        # `b2` to define `Z1`, `X1`, `Z2`, `X2`. This time, `X0` is batch_size * 2 !\n",
        "        Z1 = ...\n",
        "        X1 = ...\n",
        "        Z2 = ...\n",
        "        X2 = ...\n",
        "\n",
        "        # Predicted class (use np.argmax with axis argument)\n",
        "        pred = ...\n",
        "        acc_epoch += sum(pred == y2)\n",
        "\n",
        "        # One-hot encoding of classes in `y2` (use `np.eye`)\n",
        "        y2_one_hot = ...\n",
        "\n",
        "        # Softmax of output needed in the loss (use `np.sum` with `keepdims`)\n",
        "        softmax_X2 = ...\n",
        "\n",
        "        # Gradient of loss wrt output layer\n",
        "        LX_2 = ...\n",
        "\n",
        "        # Gradient of loss wrt weights and biases in second layer\n",
        "        # Since `LW_2` is 3-dimensional, operator `@` is not working anymore.\n",
        "        # Use `np.einsum` here.\n",
        "        LW_2 = ...\n",
        "        Lb_2 = ...\n",
        "\n",
        "        # Gradient of loss wrt first layer\n",
        "        # Use `np.einsum` again.\n",
        "        LX_1 = ...\n",
        "\n",
        "        # Gradient of loss wrt weights and biases in first layer\n",
        "        LW_1 = ...\n",
        "        Lb_1 = ...\n",
        "\n",
        "        # Gradient descent step: use `eta`, `Lw_1` `Lw_2` `Lb_1` `Lb_2` to\n",
        "        # update `W1`, `W2`, `b1`, `b2`. Don't forget to average gradients over\n",
        "        # the minibatch.\n",
        "        W1 -= ...\n",
        "        W2 -= ...\n",
        "        b1 -= ...\n",
        "        b2 -= ...\n",
        "\n",
        "    print(f\"Epoch: {epoch:02}, training accuracy: {acc_epoch/n_samples}\")"
      ],
      "id": "fbc84e42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization"
      ],
      "id": "7091451e-11f8-4b15-a237-7d31f5cb55b2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num = 250\n",
        "xx = np.linspace(X[:, 0].min(), X[:, 0].max(), num)\n",
        "yy = np.linspace(X[:, 1].min(), X[:, 1].max(), num)\n",
        "XX, YY = np.meshgrid(xx, yy)\n",
        "points = np.c_[XX.ravel(), YY.ravel()]\n",
        "\n",
        "# Forward pass on all points\n",
        "Z1 = points @ W1 + b1\n",
        "X1 = af(Z1)\n",
        "Z2 = X1 @ W2 + b2\n",
        "X2 = Z2\n",
        "X2_hat = np.argmax(Z2, axis=1)\n",
        "\n",
        "C = X2_hat.reshape(num, num)\n",
        "\n",
        "plt.contourf(XX, YY, C, cmap=plt.get_cmap(\"tab10\"), alpha=.4)\n",
        "plt.scatter(*X.T, c=y, cmap=plt.get_cmap(\"tab10\"))\n",
        "\n",
        "plt.show()"
      ],
      "id": "50125cee"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  }
}