{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656e06df-69ee-4e5d-8935-633292e94850",
   "metadata": {},
   "source": [
    "# CBOW model trained on 20000 lieues sous les mers\n",
    "\n",
    "## Needed libraries\n",
    "\n",
    "You will need the following new libraries:\n",
    "\n",
    "-   `spacy` for tokenizing\n",
    "-   `gensim` for cosine similarities (use `gensim>=4.0.0`)\n",
    "\n",
    "You will also need to download rules for tokenizing a french text.\n",
    "\n",
    "``` bash\n",
    "python -m spacy download fr_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8adcda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037c9b7-9bb7-4505-ac55-39692aa4a381",
   "metadata": {},
   "source": [
    "## Tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d919bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a french tokenizer to create a tokenizer for the french language\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "with open(\"data/20_000_lieues_sous_les_mers.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = spacy_fr.tokenizer(f.read())\n",
    "\n",
    "# Define a filtered set of tokens by iterating on `document`. Define a\n",
    "# subset of tokens that are\n",
    "#\n",
    "# - alphanumeric\n",
    "# - in lower case\n",
    "# <answer>\n",
    "tokens = [\n",
    "    tok.text.lower()\n",
    "    for tok in document if tok.is_alpha or tok.is_digit\n",
    "]\n",
    "# </answer>\n",
    "\n",
    "# Make a list of unique tokens and dictionary that maps tokens to\n",
    "# their index in that list.\n",
    "# <answer>\n",
    "idx2tok = list(set(tokens))\n",
    "tok2idx = {token: i for i, token in enumerate(idx2tok)}\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e591c-0376-4d5f-a44f-cdd17f856e74",
   "metadata": {},
   "source": [
    "## The continuous bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b54555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(14709, 32)\n",
       "  (U_transpose): Linear(in_features=32, out_features=14709, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Define an Embedding module (`nn.Embedding`) and a linear\n",
    "        # transform (`nn.Linear`) without bias.\n",
    "        # <answer>\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.U_transpose = nn.Linear(self.embedding_size, self.vocab_size, bias=False)\n",
    "        # </answer>\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Implements the forward pass of the CBOW model\n",
    "        # `context` is of size `batch_size` * NGRAMS\n",
    "\n",
    "        # `e_i` is of size `batch_size` * NGRAMS * `embedding_size`\n",
    "        # <answer>\n",
    "        e_i = self.embeddings(context)\n",
    "        # </answer>\n",
    "\n",
    "        # `e_bar` is of size `batch_size` * `embedding_size`\n",
    "        # <answer>\n",
    "        e_bar = torch.mean(e_i, dim=1)\n",
    "        # </answer>\n",
    "\n",
    "        # `UT_e_bar` is of size `batch_size` * `vocab_size`\n",
    "        # <answer>\n",
    "        UT_e_bar = self.U_transpose(e_bar)\n",
    "        # </answer>\n",
    "\n",
    "        # <answer>\n",
    "        return UT_e_bar\n",
    "        # </answer>\n",
    "\n",
    "\n",
    "# Set the size of vocabulary and size of embedding\n",
    "VOCAB_SIZE = len(idx2tok)\n",
    "EMBEDDING_SIZE = 32\n",
    "\n",
    "# Create a Continuous bag of words model\n",
    "cbow = CBOW(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "\n",
    "# Send to GPU if any\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "cbow.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf5c0e-fa2e-4901-b438-ca0334912776",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18aa66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n-grams for a given list of tokens, use yield, use window length of n-grams\n",
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Generates successive N-grams from a list of tokens.\"\"\"\n",
    "\n",
    "    for i in range(len(token_list) - ngrams + 1):\n",
    "        idxs = [tok2idx[tok] for tok in token_list[i:i+ngrams]]\n",
    "\n",
    "        # Get center element in `idxs`\n",
    "        center = idxs.pop(ngrams // 2)\n",
    "\n",
    "        # Yield the index of center word and indexes of context words\n",
    "        # as a Numpy array (for Pytorch to automatically convert it to\n",
    "        # a Tensor).\n",
    "        yield center, np.array(idxs)\n",
    "\n",
    "\n",
    "# Create center, context data\n",
    "NGRAMS = 5\n",
    "ngrams = list(ngrams_iterator(tokens, NGRAMS))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "data = torch.utils.data.DataLoader(ngrams, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9a5f7-a05a-4021-9916-aa3b5854c9bb",
   "metadata": {},
   "source": [
    "## Learn CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f6e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <answer>\n",
    "# Gradient descent algorithm to use\n",
    "# </answer>\n",
    "# <answer>\n",
    "# Use the Adam algorithm on the parameters of `cbow` with a learning\n",
    "# rate of 0.01\n",
    "# </answer>\n",
    "# <answer>\n",
    "optimizer = optim.Adam(cbow.parameters(), lr=0.01)\n",
    "# </answer>\n",
    "\n",
    "# Use a cross-entropy loss from the `nn` submodule\n",
    "# <answer>\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd071566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1/20), batch: (0/272), loss: 9.637222290039062\n",
      "Epoch (1/20), batch: (20/272), loss: 9.42026424407959\n",
      "Epoch (1/20), batch: (40/272), loss: 9.008231163024902\n",
      "Epoch (1/20), batch: (60/272), loss: 8.624564170837402\n",
      "Epoch (1/20), batch: (80/272), loss: 8.37303352355957\n",
      "Epoch (1/20), batch: (100/272), loss: 8.177963256835938\n",
      "Epoch (1/20), batch: (120/272), loss: 8.030889511108398\n",
      "Epoch (1/20), batch: (140/272), loss: 7.901767253875732\n",
      "Epoch (1/20), batch: (160/272), loss: 7.800580978393555\n",
      "Epoch (1/20), batch: (180/272), loss: 7.711447238922119\n",
      "Epoch (1/20), batch: (200/272), loss: 7.633633136749268\n",
      "Epoch (1/20), batch: (220/272), loss: 7.558661460876465\n",
      "Epoch (1/20), batch: (240/272), loss: 7.498932361602783\n",
      "Epoch (1/20), batch: (260/272), loss: 7.441467761993408\n",
      "1/20 loss 7.41\n",
      "Epoch (2/20), batch: (0/272), loss: 6.075843334197998\n",
      "Epoch (2/20), batch: (20/272), loss: 6.0872578620910645\n",
      "Epoch (2/20), batch: (40/272), loss: 6.079187393188477\n",
      "Epoch (2/20), batch: (60/272), loss: 6.063724994659424\n",
      "Epoch (2/20), batch: (80/272), loss: 6.060498237609863\n",
      "Epoch (2/20), batch: (100/272), loss: 6.0595831871032715\n",
      "Epoch (2/20), batch: (120/272), loss: 6.061919689178467\n",
      "Epoch (2/20), batch: (140/272), loss: 6.048851490020752\n",
      "Epoch (2/20), batch: (160/272), loss: 6.041501522064209\n",
      "Epoch (2/20), batch: (180/272), loss: 6.035974502563477\n",
      "Epoch (2/20), batch: (200/272), loss: 6.034740447998047\n",
      "Epoch (2/20), batch: (220/272), loss: 6.031501770019531\n",
      "Epoch (2/20), batch: (240/272), loss: 6.0290632247924805\n",
      "Epoch (2/20), batch: (260/272), loss: 6.022508144378662\n",
      "2/20 loss 6.02\n",
      "Epoch (3/20), batch: (0/272), loss: 5.473313331604004\n",
      "Epoch (3/20), batch: (20/272), loss: 5.356998920440674\n",
      "Epoch (3/20), batch: (40/272), loss: 5.379522800445557\n",
      "Epoch (3/20), batch: (60/272), loss: 5.38973331451416\n",
      "Epoch (3/20), batch: (80/272), loss: 5.3985981941223145\n",
      "Epoch (3/20), batch: (100/272), loss: 5.411922454833984\n",
      "Epoch (3/20), batch: (120/272), loss: 5.419165134429932\n",
      "Epoch (3/20), batch: (140/272), loss: 5.424374580383301\n",
      "Epoch (3/20), batch: (160/272), loss: 5.435335636138916\n",
      "Epoch (3/20), batch: (180/272), loss: 5.4413886070251465\n",
      "Epoch (3/20), batch: (200/272), loss: 5.447559356689453\n",
      "Epoch (3/20), batch: (220/272), loss: 5.452803134918213\n",
      "Epoch (3/20), batch: (240/272), loss: 5.457656383514404\n",
      "Epoch (3/20), batch: (260/272), loss: 5.4587249755859375\n",
      "3/20 loss 5.46\n",
      "Epoch (4/20), batch: (0/272), loss: 4.793727397918701\n",
      "Epoch (4/20), batch: (20/272), loss: 4.898677825927734\n",
      "Epoch (4/20), batch: (40/272), loss: 4.91618013381958\n",
      "Epoch (4/20), batch: (60/272), loss: 4.932459831237793\n",
      "Epoch (4/20), batch: (80/272), loss: 4.9563307762146\n",
      "Epoch (4/20), batch: (100/272), loss: 4.963859558105469\n",
      "Epoch (4/20), batch: (120/272), loss: 4.978475570678711\n",
      "Epoch (4/20), batch: (140/272), loss: 4.991061210632324\n",
      "Epoch (4/20), batch: (160/272), loss: 5.003859043121338\n",
      "Epoch (4/20), batch: (180/272), loss: 5.014349460601807\n",
      "Epoch (4/20), batch: (200/272), loss: 5.021384239196777\n",
      "Epoch (4/20), batch: (220/272), loss: 5.027770042419434\n",
      "Epoch (4/20), batch: (240/272), loss: 5.039220333099365\n",
      "Epoch (4/20), batch: (260/272), loss: 5.04685640335083\n",
      "4/20 loss 5.05\n",
      "Epoch (5/20), batch: (0/272), loss: 4.446185111999512\n",
      "Epoch (5/20), batch: (20/272), loss: 4.539252758026123\n",
      "Epoch (5/20), batch: (40/272), loss: 4.54265022277832\n",
      "Epoch (5/20), batch: (60/272), loss: 4.566415786743164\n",
      "Epoch (5/20), batch: (80/272), loss: 4.584414482116699\n",
      "Epoch (5/20), batch: (100/272), loss: 4.607486248016357\n",
      "Epoch (5/20), batch: (120/272), loss: 4.626537322998047\n",
      "Epoch (5/20), batch: (140/272), loss: 4.6413164138793945\n",
      "Epoch (5/20), batch: (160/272), loss: 4.653458118438721\n",
      "Epoch (5/20), batch: (180/272), loss: 4.6624298095703125\n",
      "Epoch (5/20), batch: (200/272), loss: 4.672154903411865\n",
      "Epoch (5/20), batch: (220/272), loss: 4.682568550109863\n",
      "Epoch (5/20), batch: (240/272), loss: 4.6934614181518555\n",
      "Epoch (5/20), batch: (260/272), loss: 4.703492164611816\n",
      "5/20 loss 4.71\n",
      "Epoch (6/20), batch: (0/272), loss: 4.272320747375488\n",
      "Epoch (6/20), batch: (20/272), loss: 4.203709125518799\n",
      "Epoch (6/20), batch: (40/272), loss: 4.222940921783447\n",
      "Epoch (6/20), batch: (60/272), loss: 4.250954627990723\n",
      "Epoch (6/20), batch: (80/272), loss: 4.273844242095947\n",
      "Epoch (6/20), batch: (100/272), loss: 4.294654369354248\n",
      "Epoch (6/20), batch: (120/272), loss: 4.318124771118164\n",
      "Epoch (6/20), batch: (140/272), loss: 4.329255104064941\n",
      "Epoch (6/20), batch: (160/272), loss: 4.343826770782471\n",
      "Epoch (6/20), batch: (180/272), loss: 4.353878021240234\n",
      "Epoch (6/20), batch: (200/272), loss: 4.369597911834717\n",
      "Epoch (6/20), batch: (220/272), loss: 4.386001110076904\n",
      "Epoch (6/20), batch: (240/272), loss: 4.400137424468994\n",
      "Epoch (6/20), batch: (260/272), loss: 4.4117326736450195\n",
      "6/20 loss 4.42\n",
      "Epoch (7/20), batch: (0/272), loss: 4.025113105773926\n",
      "Epoch (7/20), batch: (20/272), loss: 3.946275234222412\n",
      "Epoch (7/20), batch: (40/272), loss: 3.9729926586151123\n",
      "Epoch (7/20), batch: (60/272), loss: 3.992338180541992\n",
      "Epoch (7/20), batch: (80/272), loss: 4.008212089538574\n",
      "Epoch (7/20), batch: (100/272), loss: 4.035732269287109\n",
      "Epoch (7/20), batch: (120/272), loss: 4.056948184967041\n",
      "Epoch (7/20), batch: (140/272), loss: 4.078779220581055\n",
      "Epoch (7/20), batch: (160/272), loss: 4.091461181640625\n",
      "Epoch (7/20), batch: (180/272), loss: 4.102896690368652\n",
      "Epoch (7/20), batch: (200/272), loss: 4.118287563323975\n",
      "Epoch (7/20), batch: (220/272), loss: 4.131959915161133\n",
      "Epoch (7/20), batch: (240/272), loss: 4.14454460144043\n",
      "Epoch (7/20), batch: (260/272), loss: 4.155965805053711\n",
      "7/20 loss 4.16\n",
      "Epoch (8/20), batch: (0/272), loss: 3.6040754318237305\n",
      "Epoch (8/20), batch: (20/272), loss: 3.742706298828125\n",
      "Epoch (8/20), batch: (40/272), loss: 3.7361860275268555\n",
      "Epoch (8/20), batch: (60/272), loss: 3.762465715408325\n",
      "Epoch (8/20), batch: (80/272), loss: 3.7966926097869873\n",
      "Epoch (8/20), batch: (100/272), loss: 3.812709093093872\n",
      "Epoch (8/20), batch: (120/272), loss: 3.8291594982147217\n",
      "Epoch (8/20), batch: (140/272), loss: 3.848177433013916\n",
      "Epoch (8/20), batch: (160/272), loss: 3.8651673793792725\n",
      "Epoch (8/20), batch: (180/272), loss: 3.879896879196167\n",
      "Epoch (8/20), batch: (200/272), loss: 3.8943216800689697\n",
      "Epoch (8/20), batch: (220/272), loss: 3.9086201190948486\n",
      "Epoch (8/20), batch: (240/272), loss: 3.9206361770629883\n",
      "Epoch (8/20), batch: (260/272), loss: 3.9357593059539795\n",
      "8/20 loss 3.94\n",
      "Epoch (9/20), batch: (0/272), loss: 3.4248757362365723\n",
      "Epoch (9/20), batch: (20/272), loss: 3.5021023750305176\n",
      "Epoch (9/20), batch: (40/272), loss: 3.556443691253662\n",
      "Epoch (9/20), batch: (60/272), loss: 3.5744876861572266\n",
      "Epoch (9/20), batch: (80/272), loss: 3.6010663509368896\n",
      "Epoch (9/20), batch: (100/272), loss: 3.617514133453369\n",
      "Epoch (9/20), batch: (120/272), loss: 3.6298532485961914\n",
      "Epoch (9/20), batch: (140/272), loss: 3.6490535736083984\n",
      "Epoch (9/20), batch: (160/272), loss: 3.6680638790130615\n",
      "Epoch (9/20), batch: (180/272), loss: 3.680056095123291\n",
      "Epoch (9/20), batch: (200/272), loss: 3.691955804824829\n",
      "Epoch (9/20), batch: (220/272), loss: 3.70607852935791\n",
      "Epoch (9/20), batch: (240/272), loss: 3.7229156494140625\n",
      "Epoch (9/20), batch: (260/272), loss: 3.7339344024658203\n",
      "9/20 loss 3.74\n",
      "Epoch (10/20), batch: (0/272), loss: 3.485412120819092\n",
      "Epoch (10/20), batch: (20/272), loss: 3.309704303741455\n",
      "Epoch (10/20), batch: (40/272), loss: 3.3564305305480957\n",
      "Epoch (10/20), batch: (60/272), loss: 3.3868207931518555\n",
      "Epoch (10/20), batch: (80/272), loss: 3.415858507156372\n",
      "Epoch (10/20), batch: (100/272), loss: 3.4300687313079834\n",
      "Epoch (10/20), batch: (120/272), loss: 3.445996046066284\n",
      "Epoch (10/20), batch: (140/272), loss: 3.467061758041382\n",
      "Epoch (10/20), batch: (160/272), loss: 3.483839273452759\n",
      "Epoch (10/20), batch: (180/272), loss: 3.499972343444824\n",
      "Epoch (10/20), batch: (200/272), loss: 3.5168724060058594\n",
      "Epoch (10/20), batch: (220/272), loss: 3.5334644317626953\n",
      "Epoch (10/20), batch: (240/272), loss: 3.54766583442688\n",
      "Epoch (10/20), batch: (260/272), loss: 3.55968976020813\n",
      "10/20 loss 3.56\n",
      "Epoch (11/20), batch: (0/272), loss: 3.2837741374969482\n",
      "Epoch (11/20), batch: (20/272), loss: 3.171379804611206\n",
      "Epoch (11/20), batch: (40/272), loss: 3.1869473457336426\n",
      "Epoch (11/20), batch: (60/272), loss: 3.2251152992248535\n",
      "Epoch (11/20), batch: (80/272), loss: 3.2569501399993896\n",
      "Epoch (11/20), batch: (100/272), loss: 3.277811288833618\n",
      "Epoch (11/20), batch: (120/272), loss: 3.2935938835144043\n",
      "Epoch (11/20), batch: (140/272), loss: 3.3071885108947754\n",
      "Epoch (11/20), batch: (160/272), loss: 3.3256444931030273\n",
      "Epoch (11/20), batch: (180/272), loss: 3.341165542602539\n",
      "Epoch (11/20), batch: (200/272), loss: 3.354311227798462\n",
      "Epoch (11/20), batch: (220/272), loss: 3.3709208965301514\n",
      "Epoch (11/20), batch: (240/272), loss: 3.3881990909576416\n",
      "Epoch (11/20), batch: (260/272), loss: 3.4028139114379883\n",
      "11/20 loss 3.41\n",
      "Epoch (12/20), batch: (0/272), loss: 2.9315855503082275\n",
      "Epoch (12/20), batch: (20/272), loss: 3.0539841651916504\n",
      "Epoch (12/20), batch: (40/272), loss: 3.071948766708374\n",
      "Epoch (12/20), batch: (60/272), loss: 3.107907295227051\n",
      "Epoch (12/20), batch: (80/272), loss: 3.1350443363189697\n",
      "Epoch (12/20), batch: (100/272), loss: 3.1501853466033936\n",
      "Epoch (12/20), batch: (120/272), loss: 3.1702442169189453\n",
      "Epoch (12/20), batch: (140/272), loss: 3.183619737625122\n",
      "Epoch (12/20), batch: (160/272), loss: 3.1972694396972656\n",
      "Epoch (12/20), batch: (180/272), loss: 3.2117209434509277\n",
      "Epoch (12/20), batch: (200/272), loss: 3.226785898208618\n",
      "Epoch (12/20), batch: (220/272), loss: 3.2372303009033203\n",
      "Epoch (12/20), batch: (240/272), loss: 3.254152774810791\n",
      "Epoch (12/20), batch: (260/272), loss: 3.2684667110443115\n",
      "12/20 loss 3.27\n",
      "Epoch (13/20), batch: (0/272), loss: 2.7871005535125732\n",
      "Epoch (13/20), batch: (20/272), loss: 2.9224276542663574\n",
      "Epoch (13/20), batch: (40/272), loss: 2.952756881713867\n",
      "Epoch (13/20), batch: (60/272), loss: 2.970628261566162\n",
      "Epoch (13/20), batch: (80/272), loss: 2.9908368587493896\n",
      "Epoch (13/20), batch: (100/272), loss: 3.0065858364105225\n",
      "Epoch (13/20), batch: (120/272), loss: 3.0295162200927734\n",
      "Epoch (13/20), batch: (140/272), loss: 3.053156614303589\n",
      "Epoch (13/20), batch: (160/272), loss: 3.070152759552002\n",
      "Epoch (13/20), batch: (180/272), loss: 3.0827178955078125\n",
      "Epoch (13/20), batch: (200/272), loss: 3.099790096282959\n",
      "Epoch (13/20), batch: (220/272), loss: 3.1141457557678223\n",
      "Epoch (13/20), batch: (240/272), loss: 3.127293586730957\n",
      "Epoch (13/20), batch: (260/272), loss: 3.1389970779418945\n",
      "13/20 loss 3.15\n",
      "Epoch (14/20), batch: (0/272), loss: 2.838655710220337\n",
      "Epoch (14/20), batch: (20/272), loss: 2.8382582664489746\n",
      "Epoch (14/20), batch: (40/272), loss: 2.8641269207000732\n",
      "Epoch (14/20), batch: (60/272), loss: 2.863720655441284\n",
      "Epoch (14/20), batch: (80/272), loss: 2.8918232917785645\n",
      "Epoch (14/20), batch: (100/272), loss: 2.9089717864990234\n",
      "Epoch (14/20), batch: (120/272), loss: 2.927574872970581\n",
      "Epoch (14/20), batch: (140/272), loss: 2.9414076805114746\n",
      "Epoch (14/20), batch: (160/272), loss: 2.9557249546051025\n",
      "Epoch (14/20), batch: (180/272), loss: 2.975762367248535\n",
      "Epoch (14/20), batch: (200/272), loss: 2.9896304607391357\n",
      "Epoch (14/20), batch: (220/272), loss: 3.003201723098755\n",
      "Epoch (14/20), batch: (240/272), loss: 3.0173792839050293\n",
      "Epoch (14/20), batch: (260/272), loss: 3.0291407108306885\n",
      "14/20 loss 3.04\n",
      "Epoch (15/20), batch: (0/272), loss: 2.752669095993042\n",
      "Epoch (15/20), batch: (20/272), loss: 2.7526803016662598\n",
      "Epoch (15/20), batch: (40/272), loss: 2.768860101699829\n",
      "Epoch (15/20), batch: (60/272), loss: 2.779536008834839\n",
      "Epoch (15/20), batch: (80/272), loss: 2.790438413619995\n",
      "Epoch (15/20), batch: (100/272), loss: 2.816053867340088\n",
      "Epoch (15/20), batch: (120/272), loss: 2.8377363681793213\n",
      "Epoch (15/20), batch: (140/272), loss: 2.853666067123413\n",
      "Epoch (15/20), batch: (160/272), loss: 2.8696062564849854\n",
      "Epoch (15/20), batch: (180/272), loss: 2.885345697402954\n",
      "Epoch (15/20), batch: (200/272), loss: 2.8979196548461914\n",
      "Epoch (15/20), batch: (220/272), loss: 2.908700704574585\n",
      "Epoch (15/20), batch: (240/272), loss: 2.9167978763580322\n",
      "Epoch (15/20), batch: (260/272), loss: 2.9267163276672363\n",
      "15/20 loss 2.94\n",
      "Epoch (16/20), batch: (0/272), loss: 2.5704843997955322\n",
      "Epoch (16/20), batch: (20/272), loss: 2.6390116214752197\n",
      "Epoch (16/20), batch: (40/272), loss: 2.668323278427124\n",
      "Epoch (16/20), batch: (60/272), loss: 2.6748900413513184\n",
      "Epoch (16/20), batch: (80/272), loss: 2.707287311553955\n",
      "Epoch (16/20), batch: (100/272), loss: 2.7179360389709473\n",
      "Epoch (16/20), batch: (120/272), loss: 2.7376463413238525\n",
      "Epoch (16/20), batch: (140/272), loss: 2.7522754669189453\n",
      "Epoch (16/20), batch: (160/272), loss: 2.766324996948242\n",
      "Epoch (16/20), batch: (180/272), loss: 2.780164957046509\n",
      "Epoch (16/20), batch: (200/272), loss: 2.7960205078125\n",
      "Epoch (16/20), batch: (220/272), loss: 2.81253981590271\n",
      "Epoch (16/20), batch: (240/272), loss: 2.825390577316284\n",
      "Epoch (16/20), batch: (260/272), loss: 2.8375823497772217\n",
      "16/20 loss 2.85\n",
      "Epoch (17/20), batch: (0/272), loss: 2.7132813930511475\n",
      "Epoch (17/20), batch: (20/272), loss: 2.593675136566162\n",
      "Epoch (17/20), batch: (40/272), loss: 2.5908119678497314\n",
      "Epoch (17/20), batch: (60/272), loss: 2.6082730293273926\n",
      "Epoch (17/20), batch: (80/272), loss: 2.6278350353240967\n",
      "Epoch (17/20), batch: (100/272), loss: 2.6397173404693604\n",
      "Epoch (17/20), batch: (120/272), loss: 2.6548798084259033\n",
      "Epoch (17/20), batch: (140/272), loss: 2.6657867431640625\n",
      "Epoch (17/20), batch: (160/272), loss: 2.6843345165252686\n",
      "Epoch (17/20), batch: (180/272), loss: 2.7014570236206055\n",
      "Epoch (17/20), batch: (200/272), loss: 2.7155404090881348\n",
      "Epoch (17/20), batch: (220/272), loss: 2.7296385765075684\n",
      "Epoch (17/20), batch: (240/272), loss: 2.7413687705993652\n",
      "Epoch (17/20), batch: (260/272), loss: 2.757080554962158\n",
      "17/20 loss 2.76\n",
      "Epoch (18/20), batch: (0/272), loss: 2.377619981765747\n",
      "Epoch (18/20), batch: (20/272), loss: 2.4507651329040527\n",
      "Epoch (18/20), batch: (40/272), loss: 2.5137829780578613\n",
      "Epoch (18/20), batch: (60/272), loss: 2.5338072776794434\n",
      "Epoch (18/20), batch: (80/272), loss: 2.5505731105804443\n",
      "Epoch (18/20), batch: (100/272), loss: 2.5649828910827637\n",
      "Epoch (18/20), batch: (120/272), loss: 2.5894834995269775\n",
      "Epoch (18/20), batch: (140/272), loss: 2.6033365726470947\n",
      "Epoch (18/20), batch: (160/272), loss: 2.6223978996276855\n",
      "Epoch (18/20), batch: (180/272), loss: 2.6296818256378174\n",
      "Epoch (18/20), batch: (200/272), loss: 2.645540952682495\n",
      "Epoch (18/20), batch: (220/272), loss: 2.656834602355957\n",
      "Epoch (18/20), batch: (240/272), loss: 2.670027732849121\n",
      "Epoch (18/20), batch: (260/272), loss: 2.6851840019226074\n",
      "18/20 loss 2.69\n",
      "Epoch (19/20), batch: (0/272), loss: 2.4390671253204346\n",
      "Epoch (19/20), batch: (20/272), loss: 2.4337034225463867\n",
      "Epoch (19/20), batch: (40/272), loss: 2.465780019760132\n",
      "Epoch (19/20), batch: (60/272), loss: 2.4799017906188965\n",
      "Epoch (19/20), batch: (80/272), loss: 2.4940128326416016\n",
      "Epoch (19/20), batch: (100/272), loss: 2.509627103805542\n",
      "Epoch (19/20), batch: (120/272), loss: 2.528736114501953\n",
      "Epoch (19/20), batch: (140/272), loss: 2.5401809215545654\n",
      "Epoch (19/20), batch: (160/272), loss: 2.5526843070983887\n",
      "Epoch (19/20), batch: (180/272), loss: 2.563187599182129\n",
      "Epoch (19/20), batch: (200/272), loss: 2.5742855072021484\n",
      "Epoch (19/20), batch: (220/272), loss: 2.5920021533966064\n",
      "Epoch (19/20), batch: (240/272), loss: 2.6061058044433594\n",
      "Epoch (19/20), batch: (260/272), loss: 2.6167550086975098\n",
      "19/20 loss 2.62\n",
      "Epoch (20/20), batch: (0/272), loss: 2.5980305671691895\n",
      "Epoch (20/20), batch: (20/272), loss: 2.40671968460083\n",
      "Epoch (20/20), batch: (40/272), loss: 2.4076762199401855\n",
      "Epoch (20/20), batch: (60/272), loss: 2.425887107849121\n",
      "Epoch (20/20), batch: (80/272), loss: 2.439337730407715\n",
      "Epoch (20/20), batch: (100/272), loss: 2.4488697052001953\n",
      "Epoch (20/20), batch: (120/272), loss: 2.4634697437286377\n",
      "Epoch (20/20), batch: (140/272), loss: 2.4748029708862305\n",
      "Epoch (20/20), batch: (160/272), loss: 2.4938390254974365\n",
      "Epoch (20/20), batch: (180/272), loss: 2.503993511199951\n",
      "Epoch (20/20), batch: (200/272), loss: 2.5179049968719482\n",
      "Epoch (20/20), batch: (220/272), loss: 2.5291481018066406\n",
      "Epoch (20/20), batch: (240/272), loss: 2.54417085647583\n",
      "Epoch (20/20), batch: (260/272), loss: 2.5560805797576904\n",
      "20/20 loss 2.56"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    for i, (center, context) in enumerate(data):\n",
    "        center, context = center.to(device), context.to(device)\n",
    "\n",
    "        # Reset the gradients of the computational graph\n",
    "        # <answer>\n",
    "        cbow.zero_grad()\n",
    "        # </answer>\n",
    "\n",
    "        # Forward pass\n",
    "        # <answer>\n",
    "        UT_ebar = cbow.forward(context)\n",
    "        # </answer>\n",
    "\n",
    "        # Compute negative log-likelihood loss averaged over the\n",
    "        # mini-batch\n",
    "        # <answer>\n",
    "        loss = ce_loss(UT_ebar, center)\n",
    "        # </answer>\n",
    "\n",
    "        # Backward pass to compute gradients of each parameter\n",
    "        # <answer>\n",
    "        loss.backward()\n",
    "        # </answer>\n",
    "\n",
    "        # Gradient descent step according to the chosen optimizer\n",
    "        # <answer>\n",
    "        optimizer.step()\n",
    "        # </answer>\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            loss_avg = float(total_loss / (i + 1))\n",
    "            print(\n",
    "                f\"Epoch ({epoch}/{EPOCHS}), batch: ({i}/{len(data)}), loss: {loss_avg}\"\n",
    "            )\n",
    "\n",
    "    # Print average loss after each epoch\n",
    "    loss_avg = float(total_loss / len(data))\n",
    "    print(\"{}/{} loss {:.2f}\".format(epoch, EPOCHS, loss_avg))\n",
    "\n",
    "    # Predict if `predict_center_word` is implemented\n",
    "    try:\n",
    "        left_words = [\"le\", \"capitaine\"]\n",
    "        right_words = [\"me\", \"dit\"]\n",
    "        word = predict_center_word(word2vec, *left_words, *right_words)[0]\n",
    "        print(\" \".join(left_words + [word] + right_words))\n",
    "    except NameError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa657c-67a7-434a-b3a4-0d6f9560739c",
   "metadata": {},
   "source": [
    "## Prediction functions\n",
    "\n",
    "Now that the model is learned we can give it a context it has never seen\n",
    "and see what center word it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22008176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_center_word_idx(cbow, *context_words_idx, k=10):\n",
    "    \"\"\"Return k-best center words given indexes of context words.\"\"\"\n",
    "\n",
    "    # Create a fake minibatch containing just one example\n",
    "    # <answer>\n",
    "    fake_minibatch = torch.LongTensor(context_words_idx).unsqueeze(0).to(device)\n",
    "    # </answer>\n",
    "\n",
    "    # Forward propagate through the cbow model\n",
    "    # <answer>\n",
    "    score_center = cbow(fake_minibatch).squeeze()\n",
    "    # </answer>\n",
    "\n",
    "    # Retrieve top k-best indexes using `torch.topk`\n",
    "    # <answer>\n",
    "    _, best_idxs = torch.topk(score_center, k=k)\n",
    "    # </answer>\n",
    "\n",
    "    # Return actual tokens using `idx2tok`\n",
    "    # <answer>\n",
    "    return [idx2tok[idx] for idx in best_idxs]\n",
    "    # </answer>\n",
    "\n",
    "\n",
    "def predict_center_word(cbow, *context_words, k=10):\n",
    "    \"\"\"Return k-best center words given context words.\"\"\"\n",
    "\n",
    "    idxs = [tok2idx[tok] for tok in context_words]\n",
    "    return predict_center_word_idx(cbow, *idxs, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624a425c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['farragut',\n",
       " 'nautilus',\n",
       " 'capitaine',\n",
       " 'stewart',\n",
       " 'dernier',\n",
       " 'canot',\n",
       " 'salon',\n",
       " 'nemo',\n",
       " 'faite',\n",
       " 'compas']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_center_word(cbow, \"vingt\", \"mille\", \"sous\", \"les\")\n",
    "predict_center_word(cbow, \"mille\", \"lieues\", \"les\", \"mers\")\n",
    "predict_center_word(cbow, \"le\", \"capitaine\", \"fut\", \"le\")\n",
    "predict_center_word(cbow, \"le\", \"commandant\", \"fut\", \"le\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67a4d8-1fbd-429f-9af3-a129ea22c840",
   "metadata": {},
   "source": [
    "## Testing the embedding\n",
    "\n",
    "We use the library `gensim` to easily compute most similar words for the\n",
    "embedding we just learned. Use `gensim>=4.0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787c4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = KeyedVectors(vector_size=EMBEDDING_SIZE)\n",
    "m.add_vectors(idx2tok, cbow.embeddings.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8fd63d-1e7a-47e9-8f29-3ba30a394184",
   "metadata": {},
   "source": [
    "You can now test most similar words for, for example “lieues”, “mers”,\n",
    "“professeur”… You can look at `words_decreasing_freq` to test most\n",
    "frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f282b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, freq = np.unique(tokens, return_counts=True)\n",
    "idxs = freq.argsort()[::-1]\n",
    "words_decreasing_freq = list(zip(unique[idxs], freq[idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6694a2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sa', 0.6365803480148315),\n",
       " ('la', 0.6183558702468872),\n",
       " ('milans', 0.6031382083892822),\n",
       " ('masters', 0.5882221460342407),\n",
       " ('votre', 0.5841334462165833),\n",
       " ('contiguë', 0.5822338461875916),\n",
       " ('soi', 0.5724022388458252),\n",
       " ('attribuait', 0.5653380155563354),\n",
       " ('déterminée', 0.5650713443756104),\n",
       " ('percevoir', 0.5647618770599365)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# <answer>\n",
    "m.most_similar(\"lieues\")\n",
    "m.most_similar(\"professeur\")\n",
    "m.most_similar(\"mers\")\n",
    "m.most_similar(\"noire\")\n",
    "m.most_similar(\"mètres\")\n",
    "m.most_similar(\"ma\")\n",
    "# </answer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
