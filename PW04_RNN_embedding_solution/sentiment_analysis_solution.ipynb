{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e359a5-5869-4cca-804f-a8effd7134e4",
   "metadata": {},
   "source": [
    "# Word embedding and RNN for sentiment analysis\n",
    "\n",
    "The goal of the following notebook is to predict whether a written\n",
    "critic about a movie is positive or negative. For that we will try three\n",
    "models. A simple linear model on the word embeddings, a recurrent neural\n",
    "network and a CNN.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Libraries and Imports\n",
    "\n",
    "First some imports are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a1686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ec8b9-9739-4fa8-a954-7478945af064",
   "metadata": {},
   "source": [
    "### Global variables\n",
    "\n",
    "First letâ€™s define a few variables. `EMBEDDING_DIM` is the dimension of\n",
    "the vector space used to embed all the words of the vocabulary.\n",
    "`SEQ_LENGTH` is the maximum length of a sequence, `BATCH_SIZE` is the\n",
    "size of the batches used in stochastic optimization algorithms and\n",
    "`NUM_EPOCHS` the number of times we are going thought the entire\n",
    "training set during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c7078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <answer>\n",
    "EMBEDDING_DIM = 8\n",
    "SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e3d5c-8e61-4e2c-9649-b769e3f61f7b",
   "metadata": {},
   "source": [
    "## The `IMDb` dataset\n",
    "\n",
    "We use the `datasets` library to load the `IMDb` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6711598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']\n",
    "\n",
    "train_set[0]\n",
    "\n",
    "print(f\"Number of training examples: {len(train_set)}\")\n",
    "print(f\"Number of testing examples: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c20c9-71d3-49ac-bfb4-18f28145ef0c",
   "metadata": {},
   "source": [
    "### Building a vocabulary out of `IMDb` from a tokenizer\n",
    "\n",
    "We first need a tokenizer that takes a text a returns a list of tokens.\n",
    "There are many tokenizers available from other libraries. Here we use\n",
    "the `tokenizers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036b15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a word-level tokenizer in lower case\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = normalizers.Lowercase()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf145052-9702-4142-a851-ee0d418d9a8b",
   "metadata": {},
   "source": [
    "Then we need to define the set of words that will be understood by the\n",
    "model: this is the vocabulary. We build it from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e180e876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_texts = train_set['text']\n",
    "test_texts = test_set['text']\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=10000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "tokenizer.train_from_iterator(train_texts, trainer)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "UNK_IDX, PAD_IDX = vocab[\"[UNK]\"], vocab[\"[PAD]\"]\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "tokenizer.encode(\"All your base are belong to us\").tokens\n",
    "tokenizer.encode(\"All your base are belong to us\").ids\n",
    "\n",
    "vocab['plenty']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcf524b-b6d9-4bf9-af09-a75fa6a3c5a3",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "The training loop is decomposed into 3 different functions:\n",
    "\n",
    "-   `train_epoch`\n",
    "-   `evaluate`\n",
    "-   `train`\n",
    "\n",
    "### Collate function\n",
    "\n",
    "The collate function maps raw samples coming from the dataset to padded\n",
    "tensors of numericalized tokens ready to be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f1b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    def collate(text):\n",
    "        \"\"\"Turn a text into a tensor of integers.\"\"\"\n",
    "        ids = tokenizer.encode(text).ids[:SEQ_LENGTH]\n",
    "        return torch.LongTensor(ids)\n",
    "\n",
    "    src_batch = [collate(sample[\"text\"]) for sample in batch]\n",
    "\n",
    "    # Pad list of tensors using `pad_sequence`\n",
    "    # <answer>\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    # </answer>\n",
    "\n",
    "    # Define the labels tensor\n",
    "    # <answer>\n",
    "    tgt_batch = torch.Tensor([sample[\"label\"] for sample in batch])\n",
    "    # </answer>\n",
    "\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed276c7-7089-422b-bae7-caca5514ca9e",
   "metadata": {},
   "source": [
    "### The `accuracy` function\n",
    "\n",
    "We need to implement an accuracy function to be used in the\n",
    "`train_epoch` function (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15abefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    # `predictions` and `labels` are both tensors of same length\n",
    "\n",
    "    # Implement accuracy\n",
    "    # <answer>\n",
    "    return torch.sum((torch.sigmoid(predictions) > 0.5).float() == (labels > .5)).item() / len(\n",
    "        predictions\n",
    "    )\n",
    "    # </answer>\n",
    "\n",
    "assert accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1])) == 1\n",
    "assert accuracy(torch.Tensor([1, -2, -3]), torch.Tensor([1, 0, 1])) == 2 / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d86bb3-7b86-4f13-9752-6d198ebcc123",
   "metadata": {},
   "source": [
    "### The `train_epoch` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0303c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, optimizer: Optimizer):\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True\n",
    "    )\n",
    "\n",
    "    matches = 0\n",
    "    losses = 0\n",
    "    for sequences, labels in train_dataloader:\n",
    "        sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Implement a step of the algorithm:\n",
    "        #\n",
    "        # - set gradients to zero\n",
    "        # - forward propagate examples in `batch`\n",
    "        # - compute `loss` with chosen criterion\n",
    "        # - back-propagate gradients\n",
    "        # - gradient step\n",
    "        # <answer>\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sequences)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        # </answer>\n",
    "\n",
    "        acc = accuracy(predictions, labels)\n",
    "\n",
    "        matches += len(predictions) * acc\n",
    "\n",
    "    return losses / len(train_set), matches / len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e96b8-82ed-453b-8a1b-5ed55c1afaca",
   "metadata": {},
   "source": [
    "### The `evaluate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0c14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    losses = 0\n",
    "    matches = 0\n",
    "    for sequences, labels in val_dataloader:\n",
    "        sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        predictions = model(sequences)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        acc = accuracy(predictions, labels)\n",
    "        matches += len(predictions) * acc\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(test_set), matches / len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18d7c5-dbe8-45ea-9c60-7659b7929581",
   "metadata": {},
   "source": [
    "### The `train` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df2cd1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        start_time = timer()\n",
    "        train_loss, train_acc = train_epoch(model, optimizer)\n",
    "        end_time = timer()\n",
    "        val_loss, val_acc = evaluate(model)\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"Train loss: {train_loss:.3f}, \"\n",
    "            f\"Train acc: {train_acc:.3f}, \"\n",
    "            f\"Val loss: {val_loss:.3f}, \"\n",
    "            f\"Val acc: {val_acc:.3f}, \"\n",
    "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2f145-6db5-47d3-a457-45d7b0aa4a30",
   "metadata": {},
   "source": [
    "### Helper function to predict from a character string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb34c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    \"Predict sentiment of given sentence according to model\"\n",
    "\n",
    "    tensor, _ = collate_fn([{\"label\": 0, \"text\": sentence}])\n",
    "    model.to(DEVICE)\n",
    "    tensor = tensor.to(DEVICE)\n",
    "    prediction = model(tensor)\n",
    "    pred = torch.sigmoid(prediction)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0214e-bc33-4f68-964e-d902ea613e95",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Training a linear classifier with an embedding\n",
    "\n",
    "We first test a simple linear classifier on the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c86609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, seq_length):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Define an embedding of `vocab_size` words into a vector space\n",
    "        # of dimension `embedding_dim`.\n",
    "        # <answer>\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        # </answer>\n",
    "\n",
    "        # Define a linear layer from dimension `seq_length` *\n",
    "        # `embedding_dim` to 1.\n",
    "        # <answer>\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "        # </answer>\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is of size `seq_length` * `batch_size`\n",
    "\n",
    "        # Compute the embedding `embedded` of the batch `x`. `embedded` is\n",
    "        # of size `seq_length` * `batch_size` * `embedding_dim`\n",
    "        # <answer>\n",
    "        embedded = self.embedding(x)\n",
    "        # </answer>\n",
    "\n",
    "        # Flatten the embedded words and feed it to the linear layer. `flatten`\n",
    "        # must be of size `batch_size` * (`seq_length` * `embedding_dim`). You\n",
    "        # might need to use `permute` first.\n",
    "        # <answer>\n",
    "        flatten = embedded.permute((1, 0, 2)).reshape(-1, self.seq_length * self.embedding_dim)\n",
    "        # </answer>\n",
    "\n",
    "        # Apply the linear layer and return a squeezed version\n",
    "        # `l1` is of size `batch_size`\n",
    "        # <answer>\n",
    "        return self.l1(flatten).squeeze()\n",
    "        # </answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf549907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80513\n",
      "80513\n",
      "Epoch: 1, Train loss: 0.001, Train acc: 0.508, Val loss: 0.001, Val acc: 0.517, Epoch time = 6.517s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.541, Val loss: 0.001, Val acc: 0.528, Epoch time = 6.528s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.563, Val loss: 0.001, Val acc: 0.532, Epoch time = 6.802s\n",
      "Epoch: 4, Train loss: 0.001, Train acc: 0.579, Val loss: 0.001, Val acc: 0.542, Epoch time = 6.512s\n",
      "Epoch: 5, Train loss: 0.001, Train acc: 0.589, Val loss: 0.001, Val acc: 0.550, Epoch time = 6.567s\n",
      "Epoch: 6, Train loss: 0.001, Train acc: 0.600, Val loss: 0.001, Val acc: 0.558, Epoch time = 6.512s\n",
      "Epoch: 7, Train loss: 0.001, Train acc: 0.610, Val loss: 0.001, Val acc: 0.570, Epoch time = 6.647s\n",
      "Epoch: 8, Train loss: 0.001, Train acc: 0.625, Val loss: 0.001, Val acc: 0.578, Epoch time = 7.017s\n",
      "Epoch: 9, Train loss: 0.001, Train acc: 0.638, Val loss: 0.001, Val acc: 0.592, Epoch time = 6.775s\n",
      "Epoch: 10, Train loss: 0.001, Train acc: 0.652, Val loss: 0.001, Val acc: 0.602, Epoch time = 6.558s"
     ]
    }
   ],
   "source": [
    "embedding_net = EmbeddingNet(VOCAB_SIZE, EMBEDDING_DIM, SEQ_LENGTH)\n",
    "print(sum(torch.numel(e) for e in embedding_net.parameters() if e.requires_grad))\n",
    "\n",
    "print(\n",
    "    VOCAB_SIZE * EMBEDDING_DIM + # Embeddings\n",
    "    (SEQ_LENGTH * EMBEDDING_DIM + 1) # Linear\n",
    ")\n",
    "\n",
    "optimizer = Adam(embedding_net.parameters())\n",
    "train(embedding_net, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52f449-e7f1-4f6f-a1c4-542965c4816a",
   "metadata": {},
   "source": [
    "### Training a linear classifier with a pretrained embedding\n",
    "\n",
    "Load a GloVe pretrained embedding instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e96366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GloVe word embedding\n",
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "unknown_vector = glove_vectors.get_mean_vector(glove_vectors.index_to_key)\n",
    "vocab_vectors = torch.tensor(np.stack([glove_vectors[e] if e in glove_vectors else unknown_vector for e in vocab.keys()]))\n",
    "\n",
    "class GloVeEmbeddingNet(nn.Module):\n",
    "    def __init__(self, seq_length, vocab_vectors, freeze=True):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Define `embedding_dim` from vocabulary and the pretrained `embedding`.\n",
    "        # <answer>\n",
    "        self.embedding_dim = vocab_vectors.size(1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "        # </answer>\n",
    "\n",
    "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Same forward as in `EmbeddingNet`\n",
    "        # `x` is of size `batch_size` * `seq_length`\n",
    "        # <answer>\n",
    "\n",
    "        # `embedded` is of size `seq_length` * `batch_size` * `embedding_dim`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # `flatten` is of size `batch_size` * `(seq_length * embedding_dim)`\n",
    "        flatten = embedded.permute((1, 0, 2)).reshape(-1, self.seq_length * self.embedding_dim)\n",
    "        # </answer>\n",
    "\n",
    "        # L1 is of size batch_size\n",
    "        # <answer>\n",
    "        return self.l1(flatten).squeeze()\n",
    "        # </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376361e0-5f25-4571-b882-b3890f03e008",
   "metadata": {},
   "source": [
    "### Use pretrained embedding without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c147fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1601\n",
      "1601\n",
      "Epoch: 1, Train loss: 0.001, Train acc: 0.515, Val loss: 0.001, Val acc: 0.527, Epoch time = 6.832s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.569, Val loss: 0.001, Val acc: 0.540, Epoch time = 6.944s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.592, Val loss: 0.001, Val acc: 0.539, Epoch time = 7.091s\n",
      "Epoch: 4, Train loss: 0.001, Train acc: 0.598, Val loss: 0.001, Val acc: 0.542, Epoch time = 6.835s\n",
      "Epoch: 5, Train loss: 0.001, Train acc: 0.606, Val loss: 0.001, Val acc: 0.541, Epoch time = 6.761s\n",
      "Epoch: 6, Train loss: 0.001, Train acc: 0.607, Val loss: 0.001, Val acc: 0.544, Epoch time = 6.676s\n",
      "Epoch: 7, Train loss: 0.001, Train acc: 0.610, Val loss: 0.001, Val acc: 0.547, Epoch time = 6.639s\n",
      "Epoch: 8, Train loss: 0.001, Train acc: 0.611, Val loss: 0.001, Val acc: 0.545, Epoch time = 7.068s\n",
      "Epoch: 9, Train loss: 0.001, Train acc: 0.615, Val loss: 0.001, Val acc: 0.548, Epoch time = 6.632s\n",
      "Epoch: 10, Train loss: 0.001, Train acc: 0.616, Val loss: 0.001, Val acc: 0.547, Epoch time = 6.538s"
     ]
    }
   ],
   "source": [
    "glove_embedding_net_freeze = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
    "print(sum(torch.numel(e) for e in glove_embedding_net_freeze.parameters() if e.requires_grad))\n",
    "\n",
    "print(\n",
    "    (SEQ_LENGTH * 25 + 1) # Linear\n",
    ")\n",
    "\n",
    "optimizer = Adam(glove_embedding_net_freeze.parameters())\n",
    "train(glove_embedding_net_freeze, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff0b6d-ef6e-4fd4-8fbe-741768867cda",
   "metadata": {},
   "source": [
    "### Fine-tuning the pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54daf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and don't freeze embedding weights\n",
    "# <answer>\n",
    "glove_embedding_net = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=False)\n",
    "# </answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790f9c7-e872-434f-bf5d-5763cfe297ec",
   "metadata": {},
   "source": [
    "### Recurrent neural network with frozen pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f44b466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38201\n",
      "38201\n",
      "Epoch: 1, Train loss: 0.001, Train acc: 0.516, Val loss: 0.001, Val acc: 0.522, Epoch time = 12.191s\n",
      "Epoch: 2, Train loss: 0.001, Train acc: 0.536, Val loss: 0.001, Val acc: 0.554, Epoch time = 13.282s\n",
      "Epoch: 3, Train loss: 0.001, Train acc: 0.574, Val loss: 0.001, Val acc: 0.593, Epoch time = 12.237s\n",
      "Epoch: 4, Train loss: 0.001, Train acc: 0.615, Val loss: 0.001, Val acc: 0.616, Epoch time = 12.255s\n",
      "Epoch: 5, Train loss: 0.001, Train acc: 0.655, Val loss: 0.001, Val acc: 0.650, Epoch time = 12.353s\n",
      "Epoch: 6, Train loss: 0.001, Train acc: 0.674, Val loss: 0.001, Val acc: 0.656, Epoch time = 12.266s\n",
      "Epoch: 7, Train loss: 0.001, Train acc: 0.704, Val loss: 0.001, Val acc: 0.664, Epoch time = 11.766s\n",
      "Epoch: 8, Train loss: 0.001, Train acc: 0.726, Val loss: 0.001, Val acc: 0.664, Epoch time = 13.242s\n",
      "Epoch: 9, Train loss: 0.001, Train acc: 0.751, Val loss: 0.001, Val acc: 0.656, Epoch time = 11.820s\n",
      "Epoch: 10, Train loss: 0.001, Train acc: 0.793, Val loss: 0.001, Val acc: 0.655, Epoch time = 13.134s"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_vectors, freeze=True):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Define pretrained embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
    "\n",
    "        # Size of input `x_t` from `embedding`\n",
    "        self.embedding_size = self.embedding.embedding_dim\n",
    "        self.input_size = self.embedding_size\n",
    "\n",
    "        # Size of hidden state `h_t`\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define a GRU\n",
    "        # <answer>\n",
    "        self.gru = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size)\n",
    "        # </answer>\n",
    "\n",
    "        # Linear layer on last hidden state\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # `x` is of size `seq_length` * `batch_size` and `h0` is of size 1\n",
    "        # * `batch_size` * `hidden_size`\n",
    "\n",
    "        # Define first hidden state in not provided\n",
    "        if h0 is None:\n",
    "            # Get batch and define `h0` which is of size 1 * `batch_size` *\n",
    "            # `hidden_size`\n",
    "            # <answer>\n",
    "            batch_size = x.size(1)\n",
    "            h0 = torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)\n",
    "            # </answer>\n",
    "\n",
    "        # `embedded` is of size `seq_length` * `batch_size` *\n",
    "        # `embedding_dim`\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Define `output` and `hidden` returned by GRU:\n",
    "        #\n",
    "        # - `output` is of size `seq_length` * `batch_size` * `embedding_dim`\n",
    "        #   and gathers all the hidden states along the sequence.\n",
    "        # - `hidden` is of size 1 * `batch_size` * `embedding_dim` and is the\n",
    "        #   last hidden state.\n",
    "        # <answer>\n",
    "        output, hidden = self.gru(embedded, h0)\n",
    "        # </answer>\n",
    "\n",
    "        # Apply a linear layer on the last hidden state to have a score tensor\n",
    "        # of size 1 * `batch_size` * 1, and return a one-dimensional tensor of\n",
    "        # size `batch_size`.\n",
    "        # <answer>\n",
    "        return self.linear(hidden).squeeze()\n",
    "        # </answer>\n",
    "\n",
    "\n",
    "rnn = RNN(hidden_size=100, vocab_vectors=vocab_vectors)\n",
    "print(sum(torch.numel(e) for e in rnn.parameters() if e.requires_grad))\n",
    "\n",
    "hidden_size = 100\n",
    "print(\n",
    "    3 * hidden_size * (hidden_size + 25 + 2) + # GRU (2 bias vectors instead of 1)\n",
    "    hidden_size + 1 # Linear\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.005)\n",
    "train(rnn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
