{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word embedding and RNN for sentiment analysis\n",
        "\n",
        "The goal of the following notebook is to predict whether a written\n",
        "critic about a movie is positive or negative. For that we will try three\n",
        "models. A simple linear model on the word embeddings, a recurrent neural\n",
        "network and a CNN.\n",
        "\n",
        "## Preliminaries\n",
        "\n",
        "### Libraries and Imports\n",
        "\n",
        "First some imports are needed."
      ],
      "id": "af221498-e941-4428-bfd2-59f7ef5420a1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam, Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers"
      ],
      "id": "b1cdd4f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Global variables\n",
        "\n",
        "First letâ€™s define a few variables. `EMBEDDING_DIM` is the dimension of\n",
        "the vector space used to embed all the words of the vocabulary.\n",
        "`SEQ_LENGTH` is the maximum length of a sequence, `BATCH_SIZE` is the\n",
        "size of the batches used in stochastic optimization algorithms and\n",
        "`NUM_EPOCHS` the number of times we are going thought the entire\n",
        "training set during the training phase."
      ],
      "id": "4c36a5b0-a957-4b71-b82c-d498390baf3f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = ...\n",
        "SEQ_LENGTH = ...\n",
        "BATCH_SIZE = ...\n",
        "NUM_EPOCHS = ...\n",
        "DEVICE = ..."
      ],
      "id": "40b1c37a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The `IMDb` dataset\n",
        "\n",
        "We use the `datasets` library to load the `IMDb` dataset."
      ],
      "id": "c176c916-bc61-4382-ba13-67651770328b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "train_set = dataset['train']\n",
        "test_set = dataset['test']\n",
        "\n",
        "train_set[0]\n",
        "\n",
        "print(f\"Number of training examples: {len(train_set)}\")\n",
        "print(f\"Number of testing examples: {len(test_set)}\")"
      ],
      "id": "7934ce80"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building a vocabulary out of `IMDb` from a tokenizer\n",
        "\n",
        "We first need a tokenizer that takes a text a returns a list of tokens.\n",
        "There are many tokenizers available from other libraries. Here we use\n",
        "the `tokenizers` library."
      ],
      "id": "0b130e41-531c-4389-9f3a-86195ec82fc0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use a word-level tokenizer in lower case\n",
        "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = normalizers.Lowercase()\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
      ],
      "id": "218d932c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we need to define the set of words that will be understood by the\n",
        "model: this is the vocabulary. We build it from the training set."
      ],
      "id": "b612e77e-8d85-44e7-b613-ca3bc2fab800"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_texts = train_set['text']\n",
        "test_texts = test_set['text']\n",
        "\n",
        "trainer = trainers.WordLevelTrainer(vocab_size=10000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "tokenizer.train_from_iterator(train_texts, trainer)\n",
        "\n",
        "vocab = tokenizer.get_vocab()\n",
        "\n",
        "UNK_IDX, PAD_IDX = vocab[\"[UNK]\"], vocab[\"[PAD]\"]\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "tokenizer.encode(\"All your base are belong to us\").tokens\n",
        "tokenizer.encode(\"All your base are belong to us\").ids\n",
        "\n",
        "vocab['plenty']"
      ],
      "id": "28ee601b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The training loop\n",
        "\n",
        "The training loop is decomposed into 3 different functions:\n",
        "\n",
        "-   `train_epoch`\n",
        "-   `evaluate`\n",
        "-   `train`\n",
        "\n",
        "### Collate function\n",
        "\n",
        "The collate function maps raw samples coming from the dataset to padded\n",
        "tensors of numericalized tokens ready to be fed to the model."
      ],
      "id": "d897cee6-476d-4407-babc-5fd893b42d02"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    def collate(text):\n",
        "        \"\"\"Turn a text into a tensor of integers.\"\"\"\n",
        "        ids = tokenizer.encode(text).ids[:SEQ_LENGTH]\n",
        "        return torch.LongTensor(ids)\n",
        "\n",
        "    src_batch = [collate(sample[\"text\"]) for sample in batch]\n",
        "\n",
        "    # Pad list of tensors using `pad_sequence`\n",
        "    src_batch = ...\n",
        "\n",
        "    # Define the labels tensor\n",
        "    tgt_batch = ...\n",
        "\n",
        "    return src_batch, tgt_batch"
      ],
      "id": "e9f41d2c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The `accuracy` function\n",
        "\n",
        "We need to implement an accuracy function to be used in the\n",
        "`train_epoch` function (see below)."
      ],
      "id": "bb78473a-e31b-47f5-b5f8-d716d86a8e40"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(predictions, labels):\n",
        "    # `predictions` and `labels` are both tensors of same length\n",
        "\n",
        "    # Implement accuracy\n",
        "    return ...\n",
        "\n",
        "assert accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1])) == 1\n",
        "assert accuracy(torch.Tensor([1, -2, -3]), torch.Tensor([1, 0, 1])) == 2 / 3"
      ],
      "id": "3372c02c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The `train_epoch` function"
      ],
      "id": "a1bb4b73-a3e9-4bed-97c6-f966d501b190"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model: nn.Module, optimizer: Optimizer):\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # Training mode\n",
        "    model.train()\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True\n",
        "    )\n",
        "\n",
        "    matches = 0\n",
        "    losses = 0\n",
        "    for sequences, labels in train_dataloader:\n",
        "        sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Implement a step of the algorithm:\n",
        "        #\n",
        "        # - set gradients to zero\n",
        "        # - forward propagate examples in `batch`\n",
        "        # - compute `loss` with chosen criterion\n",
        "        # - back-propagate gradients\n",
        "        # - gradient step\n",
        "        ...\n",
        "\n",
        "        acc = accuracy(predictions, labels)\n",
        "\n",
        "        matches += len(predictions) * acc\n",
        "\n",
        "    return losses / len(train_set), matches / len(train_set)"
      ],
      "id": "d961e906"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The `evaluate` function"
      ],
      "id": "37baabc9-d96e-43f6-a93e-fbf42b26fd2d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module):\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    losses = 0\n",
        "    matches = 0\n",
        "    for sequences, labels in val_dataloader:\n",
        "        sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        predictions = model(sequences)\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        acc = accuracy(predictions, labels)\n",
        "        matches += len(predictions) * acc\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(test_set), matches / len(test_set)"
      ],
      "id": "a037db8c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The `train` function"
      ],
      "id": "660f4cff-6cb9-4e28-99b6-39af1ec292a7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, optimizer):\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        start_time = timer()\n",
        "        train_loss, train_acc = train_epoch(model, optimizer)\n",
        "        end_time = timer()\n",
        "        val_loss, val_acc = evaluate(model)\n",
        "        print(\n",
        "            f\"Epoch: {epoch}, \"\n",
        "            f\"Train loss: {train_loss:.3f}, \"\n",
        "            f\"Train acc: {train_acc:.3f}, \"\n",
        "            f\"Val loss: {val_loss:.3f}, \"\n",
        "            f\"Val acc: {val_acc:.3f}, \"\n",
        "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
        "        )"
      ],
      "id": "f1ac3bd3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function to predict from a character string"
      ],
      "id": "f2c57de6-3fa3-4a1a-afc4-c9d3047861cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(model, sentence):\n",
        "    \"Predict sentiment of given sentence according to model\"\n",
        "\n",
        "    tensor, _ = collate_fn([{\"label\": 0, \"text\": sentence}])\n",
        "    model.to(DEVICE)\n",
        "    tensor = tensor.to(DEVICE)\n",
        "    prediction = model(tensor)\n",
        "    pred = torch.sigmoid(prediction)\n",
        "    return pred.item()"
      ],
      "id": "db934ca4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models\n",
        "\n",
        "### Training a linear classifier with an embedding\n",
        "\n",
        "We first test a simple linear classifier on the word embeddings."
      ],
      "id": "16da4875-79e0-4ed9-9365-5d59a5c066dc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, seq_length):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Define an embedding of `vocab_size` words into a vector space\n",
        "        # of dimension `embedding_dim`.\n",
        "        self.embedding = ...\n",
        "\n",
        "        # Define a linear layer from dimension `seq_length` *\n",
        "        # `embedding_dim` to 1.\n",
        "        self.l1 = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # `x` is of size `seq_length` * `batch_size`\n",
        "\n",
        "        # Compute the embedding `embedded` of the batch `x`. `embedded` is\n",
        "        # of size `seq_length` * `batch_size` * `embedding_dim`\n",
        "        embedded = ...\n",
        "\n",
        "        # Flatten the embedded words and feed it to the linear layer. `flatten`\n",
        "        # must be of size `batch_size` * (`seq_length` * `embedding_dim`). You\n",
        "        # might need to use `permute` first.\n",
        "        flatten = ...\n",
        "\n",
        "        # Apply the linear layer and return a squeezed version\n",
        "        # `l1` is of size `batch_size`\n",
        "        return ..."
      ],
      "id": "49b37996"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_net = EmbeddingNet(VOCAB_SIZE, EMBEDDING_DIM, SEQ_LENGTH)\n",
        "print(sum(torch.numel(e) for e in embedding_net.parameters() if e.requires_grad))\n",
        "\n",
        "print(\n",
        "    VOCAB_SIZE * EMBEDDING_DIM + # Embeddings\n",
        "    (SEQ_LENGTH * EMBEDDING_DIM + 1) # Linear\n",
        ")\n",
        "\n",
        "optimizer = Adam(embedding_net.parameters())\n",
        "train(embedding_net, optimizer)"
      ],
      "id": "999cef91"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training a linear classifier with a pretrained embedding\n",
        "\n",
        "Load a GloVe pretrained embedding instead"
      ],
      "id": "9f9ca5d7-7e8b-452c-b0c8-0ae2a8914bf8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download GloVe word embedding\n",
        "import gensim.downloader\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "\n",
        "unknown_vector = glove_vectors.get_mean_vector(glove_vectors.index_to_key)\n",
        "vocab_vectors = torch.tensor(np.stack([glove_vectors[e] if e in glove_vectors else unknown_vector for e in vocab.keys()]))\n",
        "\n",
        "class GloVeEmbeddingNet(nn.Module):\n",
        "    def __init__(self, seq_length, vocab_vectors, freeze=True):\n",
        "        super().__init__()\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Define `embedding_dim` from vocabulary and the pretrained `embedding`.\n",
        "        self.embedding_dim = ...\n",
        "        self.embedding = ...\n",
        "\n",
        "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Same forward as in `EmbeddingNet`\n",
        "        # `x` is of size `batch_size` * `seq_length`\n",
        "        embedded = ...\n",
        "        flatten = ...\n",
        "\n",
        "        # L1 is of size batch_size\n",
        "        return ..."
      ],
      "id": "33d7d8cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use pretrained embedding without fine-tuning"
      ],
      "id": "4686d8c2-4dcd-4fc2-9330-22c131bc17ec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_embedding_net_freeze = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
        "print(sum(torch.numel(e) for e in glove_embedding_net_freeze.parameters() if e.requires_grad))\n",
        "\n",
        "print(\n",
        "    (SEQ_LENGTH * 25 + 1) # Linear\n",
        ")\n",
        "\n",
        "optimizer = Adam(glove_embedding_net_freeze.parameters())\n",
        "train(glove_embedding_net_freeze, optimizer)"
      ],
      "id": "67da3447"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine-tuning the pretrained embedding"
      ],
      "id": "be6d17e0-51d2-4d6a-9241-3c18d4ab209c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model and don't freeze embedding weights\n",
        "glove_embedding_net = ..."
      ],
      "id": "08f9f2ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recurrent neural network with frozen pretrained embedding"
      ],
      "id": "0620953b-36be-4f11-b1a6-ae13fe68a2d6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_vectors, freeze=True):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        # Define pretrained embedding\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
        "\n",
        "        # Size of input `x_t` from `embedding`\n",
        "        self.embedding_size = self.embedding.embedding_dim\n",
        "        self.input_size = self.embedding_size\n",
        "\n",
        "        # Size of hidden state `h_t`\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define a GRU\n",
        "        self.gru = ...\n",
        "\n",
        "        # Linear layer on last hidden state\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        # `x` is of size `seq_length` * `batch_size` and `h0` is of size 1\n",
        "        # * `batch_size` * `hidden_size`\n",
        "\n",
        "        # Define first hidden state in not provided\n",
        "        if h0 is None:\n",
        "            # Get batch and define `h0` which is of size 1 * `batch_size` *\n",
        "            # `hidden_size`\n",
        "            batch_size = ...\n",
        "            h0 = ...\n",
        "\n",
        "        # `embedded` is of size `seq_length` * `batch_size` *\n",
        "        # `embedding_dim`\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Define `output` and `hidden` returned by GRU:\n",
        "        #\n",
        "        # - `output` is of size `seq_length` * `batch_size` * `embedding_dim`\n",
        "        #   and gathers all the hidden states along the sequence.\n",
        "        # - `hidden` is of size 1 * `batch_size` * `embedding_dim` and is the\n",
        "        #   last hidden state.\n",
        "        output, hidden = ...\n",
        "\n",
        "        # Apply a linear layer on the last hidden state to have a score tensor\n",
        "        # of size 1 * `batch_size` * 1, and return a one-dimensional tensor of\n",
        "        # size `batch_size`.\n",
        "        return ...\n",
        "\n",
        "\n",
        "rnn = RNN(hidden_size=100, vocab_vectors=vocab_vectors)\n",
        "print(sum(torch.numel(e) for e in rnn.parameters() if e.requires_grad))\n",
        "\n",
        "hidden_size = 100\n",
        "print(\n",
        "    3 * hidden_size * (hidden_size + 25 + 2) + # GRU (2 bias vectors instead of 1)\n",
        "    hidden_size + 1 # Linear\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=0.005)\n",
        "train(rnn, optimizer)"
      ],
      "id": "59420d97"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/sylvain/.local/share/jupyter/kernels/python3"
    }
  }
}