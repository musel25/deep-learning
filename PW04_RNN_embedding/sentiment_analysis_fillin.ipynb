{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "af221498-e941-4428-bfd2-59f7ef5420a1",
      "metadata": {},
      "source": [
        "# Word embedding and RNN for sentiment analysis\n",
        "\n",
        "The goal of the following notebook is to predict whether a written\n",
        "critic about a movie is positive or negative. For that we will try three\n",
        "models. A simple linear model on the word embeddings, a recurrent neural\n",
        "network and a CNN.\n",
        "\n",
        "## Preliminaries\n",
        "\n",
        "### Libraries and Imports\n",
        "\n",
        "First some imports are needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b1cdd4f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/musel/Documents/github/deep-learning/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam, Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c36a5b0-a957-4b71-b82c-d498390baf3f",
      "metadata": {},
      "source": [
        "### Global variables\n",
        "\n",
        "First letâ€™s define a few variables. `EMBEDDING_DIM` is the dimension of\n",
        "the vector space used to embed all the words of the vocabulary.\n",
        "`SEQ_LENGTH` is the maximum length of a sequence, `BATCH_SIZE` is the\n",
        "size of the batches used in stochastic optimization algorithms and\n",
        "`NUM_EPOCHS` the number of times we are going thought the entire\n",
        "training set during the training phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "40b1c37a",
      "metadata": {},
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 8\n",
        "SEQ_LENGTH = 54\n",
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 3\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c176c916-bc61-4382-ba13-67651770328b",
      "metadata": {},
      "source": [
        "## The `IMDb` dataset\n",
        "\n",
        "We use the `datasets` library to load the `IMDb` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7934ce80",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "train_set = dataset['train']\n",
        "test_set = dataset['test']\n",
        "\n",
        "train_set[0]\n",
        "\n",
        "print(f\"Number of training examples: {len(train_set)}\")\n",
        "print(f\"Number of testing examples: {len(test_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b130e41-531c-4389-9f3a-86195ec82fc0",
      "metadata": {},
      "source": [
        "### Building a vocabulary out of `IMDb` from a tokenizer\n",
        "\n",
        "We first need a tokenizer that takes a text a returns a list of tokens.\n",
        "There are many tokenizers available from other libraries. Here we use\n",
        "the `tokenizers` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "218d932c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use a word-level tokenizer in lower case\n",
        "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = normalizers.Lowercase()\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b612e77e-8d85-44e7-b613-ca3bc2fab800",
      "metadata": {},
      "source": [
        "Then we need to define the set of words that will be understood by the\n",
        "model: this is the vocabulary. We build it from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "28ee601b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "988"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_texts = train_set['text']\n",
        "test_texts = test_set['text']\n",
        "\n",
        "trainer = trainers.WordLevelTrainer(vocab_size=10000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "tokenizer.train_from_iterator(train_texts, trainer)\n",
        "\n",
        "vocab = tokenizer.get_vocab()\n",
        "\n",
        "UNK_IDX, PAD_IDX = vocab[\"[UNK]\"], vocab[\"[PAD]\"]\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "tokenizer.encode(\"All your base are belong to us\").tokens\n",
        "tokenizer.encode(\"All your base are belong to us\").ids\n",
        "\n",
        "vocab['plenty']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "64a58d31",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d897cee6-476d-4407-babc-5fd893b42d02",
      "metadata": {},
      "source": [
        "## The training loop\n",
        "\n",
        "The training loop is decomposed into 3 different functions:\n",
        "\n",
        "-   `train_epoch`\n",
        "-   `evaluate`\n",
        "-   `train`\n",
        "\n",
        "### Collate function\n",
        "\n",
        "The collate function maps raw samples coming from the dataset to padded\n",
        "tensors of numericalized tokens ready to be fed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e9f41d2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    def collate(text):\n",
        "        \"\"\"Turn a text into a tensor of integers.\"\"\"\n",
        "        ids = tokenizer.encode(text).ids[:SEQ_LENGTH]\n",
        "        return torch.LongTensor(ids)\n",
        "\n",
        "    src_batch = [collate(sample[\"text\"]) for sample in batch]\n",
        "\n",
        "    # Pad list of tensors using `pad_sequence`\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "\n",
        "    # Define the labels tensor\n",
        "    tgt_batch = torch.Tensor([sample[\"label\"] for sample in batch])\n",
        "\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb78473a-e31b-47f5-b5f8-d716d86a8e40",
      "metadata": {},
      "source": [
        "### The `accuracy` function\n",
        "\n",
        "We need to implement an accuracy function to be used in the\n",
        "`train_epoch` function (see below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3372c02c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(predictions, labels):\n",
        "    # `predictions` and `labels` are both tensors of same length\n",
        "    return torch.sum((torch.sigmoid(predictions) > 0.5).float() == (labels > .5)).item() / len(\n",
        "        predictions\n",
        "    )\n",
        "\n",
        "\n",
        "assert accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1])) == 1\n",
        "assert accuracy(torch.Tensor([1, -2, -3]), torch.Tensor([1, 0, 1])) == 2 / 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "58cc9cc0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy(torch.Tensor([1, -2, 3]), torch.Tensor([1, 0, 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1bb4b73-a3e9-4bed-97c6-f966d501b190",
      "metadata": {},
      "source": [
        "### The `train_epoch` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d961e906",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model: nn.Module, optimizer: Optimizer):\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # Training mode\n",
        "    model.train()\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True\n",
        "    )\n",
        "\n",
        "    matches = 0\n",
        "    losses = 0\n",
        "    for sequences, labels in train_dataloader:\n",
        "        sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Implement a step of the algorithm:\n",
        "        #\n",
        "        # - set gradients to zero\n",
        "        # - forward propagate examples in `batch`\n",
        "        # - compute `loss` with chosen criterion\n",
        "        # - back-propagate gradients\n",
        "        # - gradient step\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(sequences)\n",
        "        loss = loss_fn(predictions,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses += loss.item()\n",
        "\n",
        "        acc = accuracy(predictions, labels)\n",
        "\n",
        "        matches += len(predictions) * acc\n",
        "\n",
        "    return losses / len(train_set), matches / len(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37baabc9-d96e-43f6-a93e-fbf42b26fd2d",
      "metadata": {},
      "source": [
        "### The `evaluate` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a037db8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module):\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    losses = 0\n",
        "    matches = 0\n",
        "    for sequences, labels in val_dataloader:\n",
        "        sequences, labels = sequences.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        predictions = model(sequences)\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        acc = accuracy(predictions, labels)\n",
        "        matches += len(predictions) * acc\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(test_set), matches / len(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "660f4cff-6cb9-4e28-99b6-39af1ec292a7",
      "metadata": {},
      "source": [
        "### The `train` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f1ac3bd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, optimizer):\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        start_time = timer()\n",
        "        train_loss, train_acc = train_epoch(model, optimizer)\n",
        "        end_time = timer()\n",
        "        val_loss, val_acc = evaluate(model)\n",
        "        print(\n",
        "            f\"Epoch: {epoch}, \"\n",
        "            f\"Train loss: {train_loss:.3f}, \"\n",
        "            f\"Train acc: {train_acc:.3f}, \"\n",
        "            f\"Val loss: {val_loss:.3f}, \"\n",
        "            f\"Val acc: {val_acc:.3f}, \"\n",
        "            f\"Epoch time = {(end_time - start_time):.3f}s\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c57de6-3fa3-4a1a-afc4-c9d3047861cd",
      "metadata": {},
      "source": [
        "### Helper function to predict from a character string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "db934ca4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(model, sentence):\n",
        "    \"Predict sentiment of given sentence according to model\"\n",
        "\n",
        "    tensor, _ = collate_fn([{\"label\": 0, \"text\": sentence}])\n",
        "    model.to(DEVICE)\n",
        "    tensor = tensor.to(DEVICE)\n",
        "    prediction = model(tensor)\n",
        "    pred = torch.sigmoid(prediction)\n",
        "    return pred.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16da4875-79e0-4ed9-9365-5d59a5c066dc",
      "metadata": {},
      "source": [
        "## Models\n",
        "\n",
        "### Training a linear classifier with an embedding\n",
        "\n",
        "We first test a simple linear classifier on the word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "49b37996",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, seq_length):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Define an embedding of `vocab_size` words into a vector space\n",
        "        # of dimension `embedding_dim`.\n",
        "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "        # Define a linear layer from dimension `seq_length` *\n",
        "        # `embedding_dim` to 1.\n",
        "        self.l1 = nn.Linear(seq_length*embedding_dim,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # `x` is of size `seq_length` * `batch_size`\n",
        "\n",
        "        # Compute the embedding `embedded` of the batch `x`. `embedded` is\n",
        "        # of size `seq_length` * `batch_size` * `embedding_dim`\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Flatten the embedded words and feed it to the linear layer. `flatten`\n",
        "        # must be of size `batch_size` * (`seq_length` * `embedding_dim`). You\n",
        "        # might need to use `permute` first.\n",
        "        flatten = (embedded.permute((1, 0, 2))).reshape((-1, self.seq_length * self.embedding_dim))\n",
        "\n",
        "        # Apply the linear layer and return a squeezed version\n",
        "        # `l1` is of size `batch_size`\n",
        "        return self.l1(flatten).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "999cef91",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80433\n",
            "80433\n",
            "Epoch: 1, Train loss: 0.001, Train acc: 0.506, Val loss: 0.001, Val acc: 0.513, Epoch time = 5.273s\n",
            "Epoch: 2, Train loss: 0.001, Train acc: 0.541, Val loss: 0.001, Val acc: 0.529, Epoch time = 5.420s\n",
            "Epoch: 3, Train loss: 0.001, Train acc: 0.563, Val loss: 0.001, Val acc: 0.542, Epoch time = 5.691s\n"
          ]
        }
      ],
      "source": [
        "embedding_net = EmbeddingNet(VOCAB_SIZE, EMBEDDING_DIM, SEQ_LENGTH)\n",
        "print(sum(torch.numel(e) for e in embedding_net.parameters() if e.requires_grad))\n",
        "\n",
        "print(\n",
        "    VOCAB_SIZE * EMBEDDING_DIM + # Embeddings\n",
        "    (SEQ_LENGTH * EMBEDDING_DIM + 1) # Linear\n",
        ")\n",
        "\n",
        "optimizer = Adam(embedding_net.parameters())\n",
        "train(embedding_net, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9ca5d7-7e8b-452c-b0c8-0ae2a8914bf8",
      "metadata": {},
      "source": [
        "### Training a linear classifier with a pretrained embedding\n",
        "\n",
        "Load a GloVe pretrained embedding instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "33d7d8cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download GloVe word embedding\n",
        "import gensim.downloader\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "\n",
        "unknown_vector = glove_vectors.get_mean_vector(glove_vectors.index_to_key)\n",
        "vocab_vectors = torch.tensor(np.stack([glove_vectors[e] if e in glove_vectors else unknown_vector for e in vocab.keys()]))\n",
        "\n",
        "class GloVeEmbeddingNet(nn.Module):\n",
        "    def __init__(self, seq_length, vocab_vectors, freeze=True):\n",
        "        super().__init__()\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Define `embedding_dim` from vocabulary and the pretrained `embedding`.\n",
        "        self.embedding_dim = vocab_vectors.size(1)\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
        "\n",
        "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Same forward as in `EmbeddingNet`\n",
        "        # `x` is of size `batch_size` * `seq_length`\n",
        "        embedded = self.embedding(x)\n",
        "        flatten = embedded.permute((1, 0, 2)).reshape(-1, self.seq_length * self.embedding_dim)\n",
        "\n",
        "        # L1 is of size batch_size\n",
        "        return self.l1(flatten).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4686d8c2-4dcd-4fc2-9330-22c131bc17ec",
      "metadata": {},
      "source": [
        "### Use pretrained embedding without fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "67da3447",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1351\n",
            "1351\n",
            "Epoch: 1, Train loss: 0.001, Train acc: 0.511, Val loss: 0.001, Val acc: 0.532, Epoch time = 5.014s\n",
            "Epoch: 2, Train loss: 0.001, Train acc: 0.562, Val loss: 0.001, Val acc: 0.541, Epoch time = 4.465s\n",
            "Epoch: 3, Train loss: 0.001, Train acc: 0.583, Val loss: 0.001, Val acc: 0.544, Epoch time = 4.425s\n"
          ]
        }
      ],
      "source": [
        "glove_embedding_net_freeze = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
        "print(sum(torch.numel(e) for e in glove_embedding_net_freeze.parameters() if e.requires_grad))\n",
        "\n",
        "print(\n",
        "    (SEQ_LENGTH * 25 + 1) # Linear\n",
        ")\n",
        "\n",
        "optimizer = Adam(glove_embedding_net_freeze.parameters())\n",
        "train(glove_embedding_net_freeze, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be6d17e0-51d2-4d6a-9241-3c18d4ab209c",
      "metadata": {},
      "source": [
        "### Fine-tuning the pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "08f9f2ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model and don't freeze embedding weights\n",
        "glove_embedding_net = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0620953b-36be-4f11-b1a6-ae13fe68a2d6",
      "metadata": {},
      "source": [
        "### Recurrent neural network with frozen pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "59420d97",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38201\n",
            "38201\n",
            "Epoch: 1, Train loss: 0.001, Train acc: 0.503, Val loss: 0.001, Val acc: 0.513, Epoch time = 18.596s\n",
            "Epoch: 2, Train loss: 0.001, Train acc: 0.534, Val loss: 0.001, Val acc: 0.554, Epoch time = 8.578s\n",
            "Epoch: 3, Train loss: 0.001, Train acc: 0.576, Val loss: 0.001, Val acc: 0.596, Epoch time = 8.953s\n"
          ]
        }
      ],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_vectors, freeze=True):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        # Define pretrained embedding\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
        "\n",
        "        # Size of input `x_t` from `embedding`\n",
        "        self.embedding_size = self.embedding.embedding_dim\n",
        "        self.input_size = self.embedding_size\n",
        "\n",
        "        # Size of hidden state `h_t`\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define a GRU\n",
        "        self.gru = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size)\n",
        "\n",
        "        # Linear layer on last hidden state\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        # `x` is of size `seq_length` * `batch_size` and `h0` is of size 1\n",
        "        # * `batch_size` * `hidden_size`\n",
        "\n",
        "        # Define first hidden state in not provided\n",
        "        if h0 is None:\n",
        "            # Get batch and define `h0` which is of size 1 * `batch_size` *\n",
        "            # `hidden_size`\n",
        "            batch_size = x.size(1)\n",
        "            h0 = torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)\n",
        "\n",
        "        # `embedded` is of size `seq_length` * `batch_size` *\n",
        "        # `embedding_dim`\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Define `output` and `hidden` returned by GRU:\n",
        "        #\n",
        "        # - `output` is of size `seq_length` * `batch_size` * `embedding_dim`\n",
        "        #   and gathers all the hidden states along the sequence.\n",
        "        # - `hidden` is of size 1 * `batch_size` * `embedding_dim` and is the\n",
        "        #   last hidden state.\n",
        "        output, hidden = self.gru(embedded,h0)\n",
        "\n",
        "        # Apply a linear layer on the last hidden state to have a score tensor\n",
        "        # of size 1 * `batch_size` * 1, and return a one-dimensional tensor of\n",
        "        # size `batch_size`.\n",
        "        return self.linear(hidden).squeeze()\n",
        "\n",
        "\n",
        "rnn = RNN(hidden_size=100, vocab_vectors=vocab_vectors)\n",
        "print(sum(torch.numel(e) for e in rnn.parameters() if e.requires_grad))\n",
        "\n",
        "hidden_size = 100\n",
        "print(\n",
        "    3 * hidden_size * (hidden_size + 25 + 2) + # GRU (2 bias vectors instead of 1)\n",
        "    hidden_size + 1 # Linear\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=0.005)\n",
        "train(rnn, optimizer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
