{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CBOW model trained on 20000 lieues sous les mers\n",
        "\n",
        "## Needed libraries\n",
        "\n",
        "You will need the following new libraries:\n",
        "\n",
        "-   `spacy` for tokenizing\n",
        "-   `gensim` for cosine similarities (use `gensim>=4.0.0`)\n",
        "\n",
        "You will also need to download rules for tokenizing a french text.\n",
        "\n",
        "``` bash\n",
        "python -m spacy download fr_core_news_sm\n",
        "```"
      ],
      "id": "a8c36c4a-f827-4d80-815a-1a63a52d533f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import spacy\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "id": "16caf452"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizing the corpus"
      ],
      "id": "2d4d23d4-fb42-4ae2-a463-d26baae65dd6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use a french tokenizer to create a tokenizer for the french language\n",
        "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
        "with open(\"data/20_000_lieues_sous_les_mers.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document = spacy_fr.tokenizer(f.read())\n",
        "\n",
        "# Define a filtered set of tokens by iterating on `document`. Define a\n",
        "# subset of tokens that are\n",
        "#\n",
        "# - alphanumeric\n",
        "# - in lower case\n",
        "tokens = ...\n",
        "\n",
        "# Make a list of unique tokens and dictionary that maps tokens to\n",
        "# their index in that list.\n",
        "idx2tok = ...\n",
        "tok2idx = ..."
      ],
      "id": "cb7760ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The continuous bag of words model"
      ],
      "id": "51ea7838-5112-4156-b3b9-fda846cf3905"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        # Define an Embedding module (`nn.Embedding`) and a linear\n",
        "        # transform (`nn.Linear`) without bias.\n",
        "        self.embeddings = ...\n",
        "        self.U_transpose = ...\n",
        "\n",
        "    def forward(self, context):\n",
        "        # Implements the forward pass of the CBOW model\n",
        "        # `context` is of size `batch_size` * NGRAMS\n",
        "\n",
        "        # `e_i` is of size `batch_size` * NGRAMS * `embedding_size`\n",
        "        e_i = ...\n",
        "\n",
        "        # `e_bar` is of size `batch_size` * `embedding_size`\n",
        "        e_bar = ...\n",
        "\n",
        "        # `UT_e_bar` is of size `batch_size` * `vocab_size`\n",
        "        UT_e_bar = ...\n",
        "\n",
        "        return ...\n",
        "\n",
        "\n",
        "# Set the size of vocabulary and size of embedding\n",
        "VOCAB_SIZE = len(idx2tok)\n",
        "EMBEDDING_SIZE = 32\n",
        "\n",
        "# Create a Continuous bag of words model\n",
        "cbow = CBOW(VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "\n",
        "# Send to GPU if any\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "cbow.to(device)"
      ],
      "id": "5629a0c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing the data"
      ],
      "id": "9b713dc9-ec5d-498f-bbc8-46cf87ae17c5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate n-grams for a given list of tokens, use yield, use window length of n-grams\n",
        "def ngrams_iterator(token_list, ngrams):\n",
        "    \"\"\"Generates successive N-grams from a list of tokens.\"\"\"\n",
        "\n",
        "    for i in range(len(token_list) - ngrams + 1):\n",
        "        idxs = [tok2idx[tok] for tok in token_list[i:i+ngrams]]\n",
        "\n",
        "        # Get center element in `idxs`\n",
        "        center = idxs.pop(ngrams // 2)\n",
        "\n",
        "        # Yield the index of center word and indexes of context words\n",
        "        # as a Numpy array (for Pytorch to automatically convert it to\n",
        "        # a Tensor).\n",
        "        yield center, np.array(idxs)\n",
        "\n",
        "\n",
        "# Create center, context data\n",
        "NGRAMS = 5\n",
        "ngrams = list(ngrams_iterator(tokens, NGRAMS))\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "data = torch.utils.data.DataLoader(ngrams, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "id": "0ad3f5cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learn CBOW model"
      ],
      "id": "5485e26c-e1c3-488f-b829-85b5f75da5b8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient descent algorithm to use\n",
        "optimizer = ...\n",
        "\n",
        "# Use a cross-entropy loss from the `nn` submodule\n",
        "ce_loss = ..."
      ],
      "id": "87c0389a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    total_loss = 0\n",
        "    for i, (center, context) in enumerate(data):\n",
        "        center, context = center.to(device), context.to(device)\n",
        "\n",
        "        # Reset the gradients of the computational graph\n",
        "        ...\n",
        "\n",
        "        # Forward pass\n",
        "        UT_ebar = ...\n",
        "\n",
        "        # Compute negative log-likelihood loss averaged over the\n",
        "        # mini-batch\n",
        "        loss = ...\n",
        "\n",
        "        # Backward pass to compute gradients of each parameter\n",
        "        ...\n",
        "\n",
        "        # Gradient descent step according to the chosen optimizer\n",
        "        ...\n",
        "\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if i % 20 == 0:\n",
        "            loss_avg = float(total_loss / (i + 1))\n",
        "            print(\n",
        "                f\"Epoch ({epoch}/{EPOCHS}), batch: ({i}/{len(data)}), loss: {loss_avg}\"\n",
        "            )\n",
        "\n",
        "    # Print average loss after each epoch\n",
        "    loss_avg = float(total_loss / len(data))\n",
        "    print(\"{}/{} loss {:.2f}\".format(epoch, EPOCHS, loss_avg))\n",
        "\n",
        "    # Predict if `predict_center_word` is implemented\n",
        "    try:\n",
        "        left_words = [\"le\", \"capitaine\"]\n",
        "        right_words = [\"me\", \"dit\"]\n",
        "        word = predict_center_word(word2vec, *left_words, *right_words)[0]\n",
        "        print(\" \".join(left_words + [word] + right_words))\n",
        "    except NameError:\n",
        "        pass"
      ],
      "id": "62e31ebf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction functions\n",
        "\n",
        "Now that the model is learned we can give it a context it has never seen\n",
        "and see what center word it predicts."
      ],
      "id": "ba7089b9-6fd2-4ad2-b65f-24e2be555c93"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_center_word_idx(cbow, *context_words_idx, k=10):\n",
        "    \"\"\"Return k-best center words given indexes of context words.\"\"\"\n",
        "\n",
        "    # Create a fake minibatch containing just one example\n",
        "    fake_minibatch = ...\n",
        "\n",
        "    # Forward propagate through the cbow model\n",
        "    score_center = ...\n",
        "\n",
        "    # Retrieve top k-best indexes using `torch.topk`\n",
        "    _, best_idxs = ...\n",
        "\n",
        "    # Return actual tokens using `idx2tok`\n",
        "    return ...\n",
        "\n",
        "\n",
        "def predict_center_word(cbow, *context_words, k=10):\n",
        "    \"\"\"Return k-best center words given context words.\"\"\"\n",
        "\n",
        "    idxs = [tok2idx[tok] for tok in context_words]\n",
        "    return predict_center_word_idx(cbow, *idxs, k=k)"
      ],
      "id": "c5fc4079"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predict_center_word(cbow, \"vingt\", \"mille\", \"sous\", \"les\")\n",
        "predict_center_word(cbow, \"mille\", \"lieues\", \"les\", \"mers\")\n",
        "predict_center_word(cbow, \"le\", \"capitaine\", \"fut\", \"le\")\n",
        "predict_center_word(cbow, \"le\", \"commandant\", \"fut\", \"le\")"
      ],
      "id": "c0bce679"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the embedding\n",
        "\n",
        "We use the library `gensim` to easily compute most similar words for the\n",
        "embedding we just learned. Use `gensim>=4.0.0`."
      ],
      "id": "85649a5c-1f2e-4628-91c9-6d57b70731de"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = KeyedVectors(vector_size=EMBEDDING_SIZE)\n",
        "m.add_vectors(idx2tok, cbow.embeddings.weight.detach().cpu().numpy())"
      ],
      "id": "b1c8c82d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now test most similar words for, for example “lieues”, “mers”,\n",
        "“professeur”… You can look at `words_decreasing_freq` to test most\n",
        "frequent tokens."
      ],
      "id": "334dd18d-39ab-427c-a368-9c9368276d18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique, freq = np.unique(tokens, return_counts=True)\n",
        "idxs = freq.argsort()[::-1]\n",
        "words_decreasing_freq = list(zip(unique[idxs], freq[idxs]))"
      ],
      "id": "18576151"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        ".most_similar(...)"
      ],
      "id": "621687ee"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/sylvain/.local/share/jupyter/kernels/python3"
    }
  }
}