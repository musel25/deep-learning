{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing a modern LeNet5\n",
        "\n",
        "## Imports"
      ],
      "id": "4f48c544-f036-4d28-aeb1-ac3d5d9376eb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "id": "82cb0b23"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and prepare the MNIST dataset"
      ],
      "id": "e499c2bd-a3de-44cd-a2bb-c9ddbbd83c16"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X, y = mnist['data'], mnist['target'].astype(int)"
      ],
      "id": "dde5df3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize and reshape\n",
        "X = X / 255.0\n",
        "X = X.reshape(-1, 1, 28, 28)  # shape: (n_samples, channels, height, width)"
      ],
      "id": "ea430b1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=42)"
      ],
      "id": "4606c7f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to Pytorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)"
      ],
      "id": "51dc41f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use dataloader to generate minibatches\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=1000, shuffle=False)"
      ],
      "id": "8a7f721c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the modern LeNet-5 model"
      ],
      "id": "2d833ac7-6bcc-42e9-a769-c7608f8dcae4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModernLeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModernLeNet5, self).__init__()\n",
        "\n",
        "        # Define a convolutional layer with 32 filters, 5x5 kernels. Adjust the\n",
        "        # padding to maintain the height and width.\n",
        "        self.conv1 = ...\n",
        "\n",
        "        # Define a batch-normalization layer\n",
        "        self.bn1 = ...\n",
        "\n",
        "        # Define a convolutional layer with 64 filters, 5x5 kernels. Adjust the\n",
        "        # padding to maintain the height and width.\n",
        "        self.conv2 = ...\n",
        "\n",
        "        # Define a batch-normalization layer\n",
        "        self.bn2 = ...\n",
        "\n",
        "        # Define a max-pooling step\n",
        "        self.pool = ...\n",
        "\n",
        "        # Define a dropout layer with dropout rate 0.25\n",
        "        self.dropout = ...\n",
        "\n",
        "        self.fc1 = ...\n",
        "        self.bn3 = ...\n",
        "        self.fc2 = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply a first convolutional layer, a batch-normalization, an\n",
        "        # activation function and a pooling\n",
        "        x = ...\n",
        "\n",
        "        # Same transformation again\n",
        "        x = ...\n",
        "\n",
        "        # Use fully connected later, batch-normalization, activation function,\n",
        "        # dropout and final fully connected layer\n",
        "        x = ...\n",
        "        x = ...\n",
        "        x = ...\n",
        "\n",
        "        return x"
      ],
      "id": "7ca8d5f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training configuration"
      ],
      "id": "69adc58e-6c95-4814-9766-0f38df40c8c0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ModernLeNet5().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "id": "90da9782"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model"
      ],
      "id": "ed793e54-d5a4-4062-b29f-f99efdfd2563"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Set gradients to zero, forward propagate, compute loss, backward\n",
        "        # propagate and update parameters\n",
        "        ...\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{epochs}] - Loss: {running_loss/len(train_loader):.4f}\")"
      ],
      "id": "70e8c31d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model"
      ],
      "id": "0740c650-9e37-4fc1-be33-74e1cf13c9bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Predict classes of `images`\n",
        "        ...\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "id": "c10df0be"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize predictions"
      ],
      "id": "d5f85f1d-cf23-4c59-9c78-0db5a75dabf4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_images, all_preds, all_labels = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_images.append(X_batch.cpu().numpy())\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(y_batch.cpu().numpy())\n",
        "\n",
        "# Stack all test data\n",
        "X_all = np.concatenate(all_images)\n",
        "y_all = np.concatenate(all_labels)\n",
        "p_all = np.concatenate(all_preds)\n",
        "\n",
        "# Find wrong predictions\n",
        "wrong_idx = np.where(p_all != y_all)[0]\n",
        "\n",
        "fig, axes = plt.subplots(5, 5, figsize=(8, 8))\n",
        "fig.suptitle(\"MNIST Wrong Predictions\", fontsize=14)\n",
        "\n",
        "for ax, idx in zip(axes.flat, wrong_idx):\n",
        "    img = X_all[idx][0]\n",
        "    true_label = y_all[idx]\n",
        "    pred_label = p_all[idx]\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(f\"T:{true_label} / P:{pred_label}\", fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "aa504ff0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus challenges\n",
        "\n",
        "1.  Compute the fraction of total model parameters that belong to the\n",
        "    classification head (i.e., the fully connected layers) in your\n",
        "    current ModernLeNet5 architecture. Modify the model to replace the\n",
        "    flattening step with a Global Average Pooling (GAP) layer before the\n",
        "    classifier. What is the new fraction relative to the total parameter\n",
        "    count?\n",
        "\n",
        "2.  Are the bias terms in a convolutional layer necessary when the layer\n",
        "    is immediately followed by a Batch Normalization layer?"
      ],
      "id": "43d04840-12a5-4870-9b96-084af245f1fa"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  }
}